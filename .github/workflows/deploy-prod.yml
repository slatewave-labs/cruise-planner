name: Deploy to Production

on:
  # Deploy when a version tag is pushed (e.g., v1.0.0)
  push:
    tags:
      - "v*"

  # Allow manual deployment
  workflow_dispatch:
    inputs:
      image_tag:
        description: "Image tag to deploy (leave empty for latest)"
        required: false
        type: string

# Cancel in-progress runs for the same environment/branch
concurrency:
  group: deploy-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  ENVIRONMENT: prod
  PROJECT_NAME: shoreexplorer
  BACKEND_ECR_REPO: shoreexplorer-backend
  FRONTEND_ECR_REPO: shoreexplorer-frontend

jobs:
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Run CI first to make sure everything passes
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ci-check:
    name: CI Validation
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Start DynamoDB Local
        run: |
          docker run -d -p 8000:8000 amazon/dynamodb-local:latest \
            -jar DynamoDBLocal.jar -sharedDb -inMemory
          echo "DynamoDB Local container started"
          sleep 5  # Give DynamoDB Local time to initialize

      - name: Wait for DynamoDB Local to be ready
        run: |
          echo "Waiting for DynamoDB Local to start..."
          pip install boto3  # Install AWS SDK for testing
          for i in {1..30}; do
            if python3 -c "import boto3; dynamodb = boto3.client('dynamodb', endpoint_url='http://localhost:8000', region_name='us-east-1', aws_access_key_id='dummy', aws_secret_access_key='dummy'); dynamodb.list_tables()" 2>/dev/null; then
              echo "DynamoDB Local is ready!"
              exit 0
            fi
            echo "Attempt $i: DynamoDB Local not ready yet, waiting..."
            sleep 2
          done
          echo "DynamoDB Local failed to start"
          docker ps -a
          docker logs $(docker ps -aq --filter ancestor=amazon/dynamodb-local:latest)
          exit 1

      - name: Install backend dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-asyncio httpx black flake8 isort

      - name: Create DynamoDB test table
        run: |
          python3 << 'EOF'
          import boto3
          
          # Create DynamoDB client
          dynamodb = boto3.client(
              'dynamodb',
              endpoint_url='http://localhost:8000',
              region_name='us-east-1',
              aws_access_key_id='dummy',
              aws_secret_access_key='dummy'
          )
          
          # Create table
          try:
              dynamodb.create_table(
                  TableName='shoreexplorer_test',
                  KeySchema=[
                      {'AttributeName': 'PK', 'KeyType': 'HASH'},
                      {'AttributeName': 'SK', 'KeyType': 'RANGE'}
                  ],
                  AttributeDefinitions=[
                      {'AttributeName': 'PK', 'AttributeType': 'S'},
                      {'AttributeName': 'SK', 'AttributeType': 'S'},
                      {'AttributeName': 'GSI1PK', 'AttributeType': 'S'},
                      {'AttributeName': 'GSI1SK', 'AttributeType': 'S'}
                  ],
                  GlobalSecondaryIndexes=[
                      {
                          'IndexName': 'GSI1',
                          'KeySchema': [
                              {'AttributeName': 'GSI1PK', 'KeyType': 'HASH'},
                              {'AttributeName': 'GSI1SK', 'KeyType': 'RANGE'}
                          ],
                          'Projection': {'ProjectionType': 'ALL'},
                          'ProvisionedThroughput': {
                              'ReadCapacityUnits': 5,
                              'WriteCapacityUnits': 5
                          }
                      }
                  ],
                  BillingMode='PROVISIONED',
                  ProvisionedThroughput={
                      'ReadCapacityUnits': 5,
                      'WriteCapacityUnits': 5
                  }
              )
              print("Table 'shoreexplorer_test' created successfully!")
          except Exception as e:
              print(f"Error creating table: {e}")
              raise
          EOF

      - name: Backend lint
        run: cd backend && black --check . && isort --check-only . && flake8 .

      - name: Backend tests
        run: |
          cd backend
          pytest tests/ -v --tb=short
        env:
          DYNAMODB_TABLE_NAME: shoreexplorer_test
          AWS_DEFAULT_REGION: us-east-1
          DYNAMODB_ENDPOINT_URL: http://localhost:8000
          AWS_ACCESS_KEY_ID: dummy
          AWS_SECRET_ACCESS_KEY: dummy
          GROQ_API_KEY: test-key-not-real

      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: yarn
          cache-dependency-path: frontend/yarn.lock

      - name: Frontend tests
        run: |
          cd frontend
          yarn install --frozen-lockfile
          yarn test --watchAll=false --ci
        env:
          CI: true

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Check if production environment is deployed
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  check-env:
    name: Check Production Environment is Deployed
    runs-on: ubuntu-latest
    environment: production
    outputs:
      deployed: ${{ steps.check.outputs.deployed }}

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check if production environment is deployed
        id: check
        run: |
          CLUSTER_STATUS=$(aws ecs describe-clusters \
            --clusters "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
            --query "clusters[0].status || 'NOT_FOUND'" \
            --output text --region "$AWS_REGION" 2>/dev/null || echo "NOT_FOUND")

          if [[ "$CLUSTER_STATUS" == "ACTIVE" ]]; then
            echo "âœ… Production environment is deployed (cluster is ACTIVE)"
            echo "deployed=true" >> "$GITHUB_OUTPUT"
          else
            echo "â­ï¸  Production environment is not deployed (cluster status: ${CLUSTER_STATUS}). Skipping deployment."
            echo "deployed=false" >> "$GITHUB_OUTPUT"
          fi

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Deploy to production (requires approval)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  deploy-prod:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [ci-check, check-env]
    environment: production  # Requires manual approval if configured

    if: needs.check-env.outputs.deployed == 'true'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set image variables
        id: vars
        env:
          GITHUB_SHA: ${{ github.sha }}
          INPUT_IMAGE_TAG: ${{ inputs.image_tag }}
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          GIT_SHA=$(echo "$GITHUB_SHA" | cut -c1-7)

          # Use provided tag or generate one
          if [[ -n "$INPUT_IMAGE_TAG" ]]; then
            IMAGE_TAG="$INPUT_IMAGE_TAG"
          else
            IMAGE_TAG="${ENVIRONMENT}-${GIT_SHA}"
          fi

          echo "ecr_base=${ECR_BASE}" >> "$GITHUB_OUTPUT"
          echo "image_tag=${IMAGE_TAG}" >> "$GITHUB_OUTPUT"
          echo "backend_image=${ECR_BASE}/${BACKEND_ECR_REPO}" >> "$GITHUB_OUTPUT"
          echo "frontend_image=${ECR_BASE}/${FRONTEND_ECR_REPO}" >> "$GITHUB_OUTPUT"

      - name: Get ALB DNS
        id: alb
        env:
          CUSTOM_DOMAIN: ${{ secrets.PROD_DOMAIN || '' }}
        run: |
          ALB_DNS=$(aws elbv2 describe-load-balancers \
            --names "${PROJECT_NAME}-${ENVIRONMENT}-alb" \
            --query "LoadBalancers[0].DNSName" \
            --output text --region "$AWS_REGION" 2>/dev/null || echo "")

          # Use custom domain if configured, otherwise use ALB DNS
          if [[ -n "$CUSTOM_DOMAIN" ]]; then
            # Production environment uses apex domain (no subdomain)
            BACKEND_URL="https://${CUSTOM_DOMAIN}"
            echo "Using custom domain: ${CUSTOM_DOMAIN}"
          elif [[ -n "$ALB_DNS" && "$ALB_DNS" != "None" ]]; then
            BACKEND_URL="https://${ALB_DNS}"
            echo "Using ALB DNS: ${ALB_DNS}"
          else
            BACKEND_URL="http://localhost:8001"
            echo "Using localhost fallback"
          fi

          echo "dns=${ALB_DNS}" >> "$GITHUB_OUTPUT"
          echo "backend_url=${BACKEND_URL}" >> "$GITHUB_OUTPUT"

      # â”€â”€ Build and push images (optimized with caching) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build and push backend image
        id: build-backend
        uses: docker/build-push-action@v5
        with:
          context: backend
          push: true
          tags: |
            ${{ steps.vars.outputs.backend_image }}:${{ steps.vars.outputs.image_tag }}
            ${{ steps.vars.outputs.backend_image }}:${{ env.ENVIRONMENT }}-latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build and push frontend image
        id: build-frontend
        uses: docker/build-push-action@v5
        with:
          context: frontend
          push: true
          build-args: |
            REACT_APP_BACKEND_URL=${{ steps.alb.outputs.backend_url }}
          tags: |
            ${{ steps.vars.outputs.frontend_image }}:${{ steps.vars.outputs.image_tag }}
            ${{ steps.vars.outputs.frontend_image }}:${{ env.ENVIRONMENT }}-latest
          cache-from: type=gha
          cache-to: type=gha,mode=max

      # â”€â”€ Deploy to ECS (rolling deployment) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      # First, register updated task definitions with correct env vars
      - name: Get secrets ARN
        id: secrets
        run: |
          SECRET_ARN=$(aws secretsmanager describe-secret \
            --secret-id "${PROJECT_NAME}-${ENVIRONMENT}-secrets" \
            --query 'ARN' --output text --region "$AWS_REGION" 2>/dev/null || echo "")
          
          if [[ -z "$SECRET_ARN" ]]; then
            echo "Warning: Could not find secrets ARN, skipping task definition update"
            echo "secret_arn=" >> "$GITHUB_OUTPUT"
          else
            echo "secret_arn=${SECRET_ARN}" >> "$GITHUB_OUTPUT"
          fi

      - name: Register backend task definition
        if: steps.secrets.outputs.secret_arn != ''
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          SECRET_ARN="${{ steps.secrets.outputs.secret_arn }}"
          
          # Get IAM role ARNs
          EXECUTION_ROLE_ARN=$(aws iam get-role \
            --role-name "${PROJECT_NAME}-${ENVIRONMENT}-ecs-task-execution-role" \
            --query 'Role.Arn' --output text --region "$AWS_REGION")
          TASK_ROLE_ARN=$(aws iam get-role \
            --role-name "${PROJECT_NAME}-${ENVIRONMENT}-ecs-task-role" \
            --query 'Role.Arn' --output text --region "$AWS_REGION")
          
          # Register backend task definition with GROQ_API_KEY
          aws ecs register-task-definition \
            --family "${PROJECT_NAME}-${ENVIRONMENT}-backend-task" \
            --network-mode awsvpc \
            --requires-compatibilities FARGATE \
            --cpu 256 \
            --memory 512 \
            --execution-role-arn "$EXECUTION_ROLE_ARN" \
            --task-role-arn "$TASK_ROLE_ARN" \
            --container-definitions "[{
              \"name\": \"backend\",
              \"image\": \"${ECR_BASE}/${BACKEND_ECR_REPO}:${ENVIRONMENT}-latest\",
              \"essential\": true,
              \"portMappings\": [{\"containerPort\": 8001, \"protocol\": \"tcp\"}],
              \"secrets\": [
                {\"name\": \"GROQ_API_KEY\", \"valueFrom\": \"${SECRET_ARN}:GROQ_API_KEY::\"}
              ],
              \"environment\": [
                {\"name\": \"DYNAMODB_TABLE_NAME\", \"value\": \"${PROJECT_NAME}-${ENVIRONMENT}\"},
                {\"name\": \"AWS_DEFAULT_REGION\", \"value\": \"${AWS_REGION}\"},
                {\"name\": \"ENVIRONMENT\", \"value\": \"${ENVIRONMENT}\"},
                {\"name\": \"ENABLE_CLOUDWATCH_METRICS\", \"value\": \"true\"}
              ],
              \"logConfiguration\": {
                \"logDriver\": \"awslogs\",
                \"options\": {
                  \"awslogs-group\": \"/ecs/${PROJECT_NAME}-${ENVIRONMENT}-backend\",
                  \"awslogs-region\": \"${AWS_REGION}\",
                  \"awslogs-stream-prefix\": \"ecs\"
                }
              },
              \"healthCheck\": {
                \"command\": [\"CMD-SHELL\", \"python -c \\\"import urllib.request; exit(0 if urllib.request.urlopen('http://localhost:8001/api/health').status == 200 else 1)\\\"\"],
                \"interval\": 30,
                \"timeout\": 10,
                \"retries\": 3,
                \"startPeriod\": 60
              }
            }]" \
            --tags "key=Project,value=${PROJECT_NAME}" "key=Environment,value=${ENVIRONMENT}" \
            --region "$AWS_REGION"

      - name: Register frontend task definition
        if: steps.secrets.outputs.secret_arn != ''
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          
          # Get IAM role ARNs
          EXECUTION_ROLE_ARN=$(aws iam get-role \
            --role-name "${PROJECT_NAME}-${ENVIRONMENT}-ecs-task-execution-role" \
            --query 'Role.Arn' --output text --region "$AWS_REGION")
          TASK_ROLE_ARN=$(aws iam get-role \
            --role-name "${PROJECT_NAME}-${ENVIRONMENT}-ecs-task-role" \
            --query 'Role.Arn' --output text --region "$AWS_REGION")
          
          # Register frontend task definition
          aws ecs register-task-definition \
            --family "${PROJECT_NAME}-${ENVIRONMENT}-frontend-task" \
            --network-mode awsvpc \
            --requires-compatibilities FARGATE \
            --cpu 256 \
            --memory 512 \
            --execution-role-arn "$EXECUTION_ROLE_ARN" \
            --task-role-arn "$TASK_ROLE_ARN" \
            --container-definitions "[{
              \"name\": \"frontend\",
              \"image\": \"${ECR_BASE}/${FRONTEND_ECR_REPO}:${ENVIRONMENT}-latest\",
              \"essential\": true,
              \"portMappings\": [{\"containerPort\": 8080, \"protocol\": \"tcp\"}],
              \"logConfiguration\": {
                \"logDriver\": \"awslogs\",
                \"options\": {
                  \"awslogs-group\": \"/ecs/${PROJECT_NAME}-${ENVIRONMENT}-frontend\",
                  \"awslogs-region\": \"${AWS_REGION}\",
                  \"awslogs-stream-prefix\": \"ecs\"
                }
              },
              \"healthCheck\": {
                \"command\": [\"CMD-SHELL\", \"wget --quiet --tries=1 --spider http://localhost:8080/ || exit 1\"],
                \"interval\": 30,
                \"timeout\": 5,
                \"retries\": 3,
                \"startPeriod\": 30
              }
            }]" \
            --tags "key=Project,value=${PROJECT_NAME}" "key=Environment,value=${ENVIRONMENT}" \
            --region "$AWS_REGION"

      - name: Update backend ECS service
        run: |
          aws ecs update-service \
            --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
            --service "${PROJECT_NAME}-${ENVIRONMENT}-backend" \
            --task-definition "${PROJECT_NAME}-${ENVIRONMENT}-backend-task" \
            --force-new-deployment \
            --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100" \
            --region "$AWS_REGION"

      - name: Ensure frontend port 8080 infrastructure
        id: frontend-infra
        run: |
          # Environments originally provisioned with port 80 need their
          # ECS security-group rule and service load-balancer config updated
          # to port 8080 (non-root nginx). This step is idempotent.

          # --- Security group: allow port 8080 from ALB ---
          ECS_SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=${PROJECT_NAME}-${ENVIRONMENT}-ecs-sg" \
            --query 'SecurityGroups[0].GroupId' --output text \
            --region "$AWS_REGION" 2>/dev/null || echo "")

          ALB_SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=${PROJECT_NAME}-${ENVIRONMENT}-alb-sg" \
            --query 'SecurityGroups[0].GroupId' --output text \
            --region "$AWS_REGION" 2>/dev/null || echo "")

          if [[ -n "$ECS_SG_ID" && "$ECS_SG_ID" != "None" && -n "$ALB_SG_ID" && "$ALB_SG_ID" != "None" ]]; then
            aws ec2 authorize-security-group-ingress \
              --group-id "$ECS_SG_ID" \
              --protocol tcp --port 8080 --source-group "$ALB_SG_ID" \
              --region "$AWS_REGION" 2>/dev/null || true
            echo "âœ… ECS security group allows port 8080 from ALB"
          else
            echo "âš ï¸  Could not find security groups â€” skipping SG update"
          fi

          # --- Retrieve frontend target group ARN from current service ---
          FRONTEND_TG_ARN=$(aws ecs describe-services \
            --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
            --services "${PROJECT_NAME}-${ENVIRONMENT}-frontend" \
            --query 'services[0].loadBalancers[0].targetGroupArn' \
            --output text --region "$AWS_REGION" 2>/dev/null || echo "")

          echo "frontend_tg_arn=${FRONTEND_TG_ARN}" >> "$GITHUB_OUTPUT"

      - name: Update frontend ECS service
        run: |
          FRONTEND_TG_ARN="${{ steps.frontend-infra.outputs.frontend_tg_arn }}"

          # Build update-service arguments
          ARGS=(
            --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster"
            --service "${PROJECT_NAME}-${ENVIRONMENT}-frontend"
            --task-definition "${PROJECT_NAME}-${ENVIRONMENT}-frontend-task"
            --force-new-deployment
            --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100"
            --region "$AWS_REGION"
          )

          # Ensure ECS registers targets on port 8080 (migrates services
          # originally created with containerPort 80)
          if [[ -n "$FRONTEND_TG_ARN" && "$FRONTEND_TG_ARN" != "None" ]]; then
            ARGS+=(--load-balancers "targetGroupArn=${FRONTEND_TG_ARN},containerName=frontend,containerPort=8080")
          fi

          aws ecs update-service "${ARGS[@]}"

      - name: Wait for deployment to stabilize
        run: |
          # aws ecs wait services-stable uses `length(deployments) == 1` which fails
          # when ECS keeps old INACTIVE deployment entries after a successful rolling
          # update (circuit-breaker or standard). We poll manually, ignoring INACTIVE
          # entries, to match what the ECS console considers "stable".
          echo "Waiting for production services to stabilize (up to 10 minutes)..."
          CLUSTER="${PROJECT_NAME}-${ENVIRONMENT}-cluster"
          SERVICES=(
            "${PROJECT_NAME}-${ENVIRONMENT}-backend"
            "${PROJECT_NAME}-${ENVIRONMENT}-frontend"
          )
          MAX_WAIT=600
          POLL_INTERVAL=15
          elapsed=0

      # â”€â”€ Async Deployment Callback (LEAN WAIT) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Provision async deployment callback infrastructure
        env:
          GH_TOKEN: ${{ secrets.GH_CALLBACK_TOKEN }}
        run: |
          # Only provision if callback token is available
          if [[ -z "$GH_TOKEN" ]]; then
            echo "â­ï¸  GH_CALLBACK_TOKEN is not set. Skipping callback infrastructure setup."
            echo "Deployment will still occur, but without the automated async callback job."
            exit 0
          fi

          echo "ğŸš€ Provisioning EventBridge-to-GitHub callback infrastructure..."
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"
          
          # 1. Connection (Auth for GitHub)
          CONN_ARN=$(aws events describe-connection --name "${APP_NAME}-gh-conn" \
            --query 'ConnectionArn' --output text --region "$AWS_REGION" 2>/dev/null || echo "")
          
          if [[ -z "$CONN_ARN" || "$CONN_ARN" == "None" ]]; then
            CONN_ARN=$(aws events create-connection --name "${APP_NAME}-gh-conn" \
              --authorization-type API_KEY \
              --auth-parameters "{\"ApiKeyAuthParameters\": {\"ApiKeyName\": \"Authorization\", \"ApiKeyValue\": \"Bearer ${GH_TOKEN}\"}}" \
              --query 'ConnectionArn' --output text --region "$AWS_REGION")
          fi

          # 2. API Destination (GitHub Repos Dispatch)
          DEST_ARN=$(aws events describe-api-destination --name "${APP_NAME}-gh-dispatch" \
            --query 'ApiDestinationArn' --output text --region "$AWS_REGION" 2>/dev/null || echo "")
          
          if [[ -z "$DEST_ARN" || "$DEST_ARN" == "None" ]]; then
            DEST_URL="https://api.github.com/repos/${{ github.repository }}/dispatches"
            DEST_ARN=$(aws events create-api-destination --name "${APP_NAME}-gh-dispatch" \
              --connection-arn "$CONN_ARN" \
              --invocation-endpoint "$DEST_URL" \
              --http-method POST \
              --query 'ApiDestinationArn' --output text --region "$AWS_REGION")
          fi

          # 3. IAM Role for EventBridge to call API Destination
          ROLE_NAME="${APP_NAME}-eventbridge-callback-role"
          if ! aws iam get-role --role-name "$ROLE_NAME" &>/dev/null; then
            TRUST_POLICY='{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"events.amazonaws.com"},"Action":"sts:AssumeRole"}]}'
            aws iam create-role --role-name "$ROLE_NAME" --assume-role-policy-document "$TRUST_POLICY" >/dev/null
            
            POLICY_DOC="{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":\"events:InvokeApiDestination\",\"Resource\":\"$DEST_ARN\"}]}"
            aws iam put-role-policy --role-name "$ROLE_NAME" --policy-name "InvokeAPIDestination" --policy-document "$POLICY_DOC"
          fi
          ROLE_ARN=$(aws iam get-role --role-name "$ROLE_NAME" --query 'Role.Arn' --output text)

          # 4. Rule (Listen for SUCCESS)
          aws events put-rule --name "${APP_NAME}-ecs-success-rule" \
            --event-pattern "{\"source\":[\"aws.ecs\"],\"detail-type\":[\"ECS Service Action\"],\"detail\":{\"clusterArn\":[\"arn:aws:ecs:${AWS_REGION}:${{ steps.vars.outputs.account_id }}:cluster/${APP_NAME}-cluster\"],\"eventName\":[\"SERVICE_STEADY_STATE\"]}}" \
            --state ENABLED --region "$AWS_REGION" --no-cli-pager >/dev/null
          
          aws events put-targets --rule "${APP_NAME}-ecs-success-rule" --region "$AWS_REGION" \
            --targets "[{\"Id\":\"GitHubSuccessDispatch\",\"Arn\":\"$DEST_ARN\",\"RoleArn\":\"$ROLE_ARN\",\"HttpParameters\":{\"QueryStringParameters\":{},\"HeaderParameters\":{\"User-Agent\":\"AWS-EventBridge\"}},\"InputTransformer\":{\"InputPathsMap\":{\"environment\":\"$.detail.clusterArn\"},\"InputTemplate\":\"{\\\"event_type\\\": \\\"ecs_deploy_success\\\", \\\"client_payload\\\": {\\\"environment\\\": \\\"${ENVIRONMENT}\\\", \\\"status\\\": \\\"stable\\\"}}\"}}]" \
            --no-cli-pager >/dev/null

          # 5. Rule (Listen for FAILURE)
          aws events put-rule --name "${APP_NAME}-ecs-failure-rule" \
            --event-pattern "{\"source\":[\"aws.ecs\"],\"detail-type\":[\"ECS Service Action\"],\"detail\":{\"clusterArn\":[\"arn:aws:ecs:${AWS_REGION}:${{ steps.vars.outputs.account_id }}:cluster/${APP_NAME}-cluster\"],\"eventName\":[\"SERVICE_DEPLOYMENT_FAILED\"]}}" \
            --state ENABLED --region "$AWS_REGION" --no-cli-pager >/dev/null
          
          aws events put-targets --rule "${APP_NAME}-ecs-failure-rule" --region "$AWS_REGION" \
            --targets "[{\"Id\":\"GitHubFailureDispatch\",\"Arn\":\"$DEST_ARN\",\"RoleArn\":\"$ROLE_ARN\",\"HttpParameters\":{\"QueryStringParameters\":{},\"HeaderParameters\":{\"User-Agent\":\"AWS-EventBridge\"}},\"InputTransformer\":{\"InputPathsMap\":{\"environment\":\"$.detail.clusterArn\"},\"InputTemplate\":\"{\\\"event_type\\\": \\\"ecs_deploy_failed\\\", \\\"client_payload\\\": {\\\"environment\\\": \\\"${ENVIRONMENT}\\\", \\\"status\\\": \\\"failed\\\"}}\"}}]" \
            --no-cli-pager >/dev/null
          echo "  âœ… Configured Success and Failure Callback Routes"

      - name: Check for callback token
        id: callback_check
        run: |
          if [[ -n "${{ secrets.GH_CALLBACK_TOKEN }}" ]]; then
            echo "has_callback=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_callback=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Wait for production services to stabilize
        if: steps.callback_check.outputs.has_callback == 'false'
        run: |
          echo "Waiting for production services to stabilize (up to 10 minutes)..."
          CLUSTER="${PROJECT_NAME}-${ENVIRONMENT}-cluster"
          SERVICES=(
            "${PROJECT_NAME}-${ENVIRONMENT}-backend"
            "${PROJECT_NAME}-${ENVIRONMENT}-frontend"
          )
          MAX_WAIT=600
          POLL_INTERVAL=15
          elapsed=0

          while [[ $elapsed -lt $MAX_WAIT ]]; do
            all_stable=true
            for svc in "${SERVICES[@]}"; do
              svc_info=$(aws ecs describe-services \
                --cluster "$CLUSTER" --services "$svc" --region "$AWS_REGION" \
                --query "services[0].{active: length(deployments[?status!='INACTIVE']), running: runningCount, desired: desiredCount}" \
                --output json)
              active=$(echo "$svc_info" | jq -r '.active')
              running=$(echo "$svc_info" | jq -r '.running')
              desired=$(echo "$svc_info" | jq -r '.desired')
              echo "${svc}: active_deployments=${active}, running=${running}, desired=${desired}"
              if [[ "$active" != "1" || "$running" != "$desired" ]]; then
                all_stable=false
              fi
            done

            if $all_stable; then
              echo "âœ… All services stable!"
              break
            fi

            sleep $POLL_INTERVAL
            elapsed=$((elapsed + POLL_INTERVAL))
          done

          if [[ $elapsed -ge $MAX_WAIT ]]; then
            echo "âŒ Timed out after ${MAX_WAIT}s waiting for services to stabilize"
            exit 1
          fi
          echo "Production deployment complete!"

      # â”€â”€ Smoke test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Run production smoke test
        if: steps.callback_check.outputs.has_callback == 'false' && steps.alb.outputs.backend_url != 'http://localhost:8001'
        env:
          BACKEND_URL: ${{ steps.alb.outputs.backend_url }}
        run: |
          echo "Running smoke tests against ${BACKEND_URL}"
          sleep 30

          # Test health endpoint
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            "${BACKEND_URL}/api/health" \
            --max-time 30 --retry 3 --retry-delay 10 || echo "000")

          if [[ "$HTTP_CODE" == "200" ]]; then
            echo "âœ… Health check passed"
          else
            echo "âŒ Health check failed (HTTP $HTTP_CODE)"
            exit 1
          fi

          # Test frontend (use same base URL without /api)
          FRONTEND_URL="${BACKEND_URL%/api*}"
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            "${FRONTEND_URL}/" \
            --max-time 30 --retry 3 --retry-delay 10 || echo "000")

          if [[ "$HTTP_CODE" == "200" ]]; then
            echo "âœ… Frontend accessible"
          else
            echo "âŒ Frontend not accessible (HTTP $HTTP_CODE)"
            exit 1
          fi

      # â”€â”€ Monitoring & Alerting â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Setup monitoring (dashboard + alarms)
        env:
          ALERT_EMAIL: ${{ secrets.ALERT_EMAIL || '' }}
        run: |
          chmod +x infra/aws/scripts/10-setup-monitoring.sh
          bash infra/aws/scripts/10-setup-monitoring.sh "${ENVIRONMENT}"
        continue-on-error: true  # Monitoring setup should not block deployment

      - name: Deployment summary
        if: always()
        env:
          BACKEND_URL: ${{ steps.alb.outputs.backend_url }}
        run: |
          echo "## Production Deployment Summary" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "| Item | Value |" >> "$GITHUB_STEP_SUMMARY"
          echo "|------|-------|" >> "$GITHUB_STEP_SUMMARY"
          echo "| Environment | \`production\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Image Tag | \`${{ steps.vars.outputs.image_tag }}\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Commit | \`${{ github.sha }}\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Tag | \`${{ github.ref_name }}\` |" >> "$GITHUB_STEP_SUMMARY"
          if [[ "${BACKEND_URL}" != "http://localhost:8001" ]]; then
            echo "| App URL | ${BACKEND_URL} |" >> "$GITHUB_STEP_SUMMARY"
            echo "| API Health | ${BACKEND_URL}/api/health |" >> "$GITHUB_STEP_SUMMARY"
            DASHBOARD_URL="https://${AWS_REGION}.console.aws.amazon.com/cloudwatch/home?region=${AWS_REGION}#dashboards:name=${PROJECT_NAME}-${ENVIRONMENT}-dashboard"
            echo "| Dashboard | [CloudWatch](${DASHBOARD_URL}) |" >> "$GITHUB_STEP_SUMMARY"
          fi

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Rollback job (manual trigger only)
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  rollback:
    name: Rollback Production
    runs-on: ubuntu-latest
    if: failure() && needs.deploy-prod.result == 'failure'
    needs: deploy-prod
    environment: production

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Rollback to previous task definition
        run: |
          echo "âš ï¸  Deployment failed. Rolling back..."

          for SERVICE in backend frontend; do
            FULL_SERVICE="${PROJECT_NAME}-${ENVIRONMENT}-${SERVICE}"
            FAMILY="${PROJECT_NAME}-${ENVIRONMENT}-${SERVICE}-task"

            # Get the previous task definition (second most recent)
            PREVIOUS_TD=$(aws ecs list-task-definitions \
              --family-prefix "$FAMILY" \
              --sort DESC \
              --query "taskDefinitionArns[1]" \
              --output text --region "$AWS_REGION")

            if [[ -n "$PREVIOUS_TD" && "$PREVIOUS_TD" != "None" ]]; then
              aws ecs update-service \
                --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
                --service "$FULL_SERVICE" \
                --task-definition "$PREVIOUS_TD" \
                --force-new-deployment \
                --region "$AWS_REGION"
              echo "Rolled back $SERVICE to: $PREVIOUS_TD"
            else
              echo "No previous task definition found for $SERVICE"
            fi
          done

      - name: Wait for rollback to stabilize
        run: |
          aws ecs wait services-stable \
            --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
            --services "${PROJECT_NAME}-${ENVIRONMENT}-backend" "${PROJECT_NAME}-${ENVIRONMENT}-frontend" \
            --region "$AWS_REGION"
          echo "Rollback complete."
