name: Deploy to Production

on:
  # Deploy when a version tag is pushed (e.g., v1.0.0)
  push:
    tags:
      - "v*"

  # Allow manual deployment
  workflow_dispatch:
    inputs:
      image_tag:
        description: "Image tag to deploy (leave empty for latest)"
        required: false
        type: string

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  ENVIRONMENT: prod
  PROJECT_NAME: shoreexplorer
  BACKEND_ECR_REPO: shoreexplorer-backend
  FRONTEND_ECR_REPO: shoreexplorer-frontend

jobs:
  # ─────────────────────────────────────────────────
  # Run CI first to make sure everything passes
  # ─────────────────────────────────────────────────
  ci-check:
    name: CI Validation
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: pip

      - name: Start DynamoDB Local
        run: |
          docker run -d -p 8000:8000 amazon/dynamodb-local:latest \
            -jar DynamoDBLocal.jar -sharedDb -inMemory
          echo "DynamoDB Local container started"
          sleep 5  # Give DynamoDB Local time to initialize

      - name: Wait for DynamoDB Local to be ready
        run: |
          echo "Waiting for DynamoDB Local to start..."
          pip install boto3  # Install AWS SDK for testing
          for i in {1..30}; do
            if python3 -c "import boto3; dynamodb = boto3.client('dynamodb', endpoint_url='http://localhost:8000', region_name='us-east-1', aws_access_key_id='dummy', aws_secret_access_key='dummy'); dynamodb.list_tables()" 2>/dev/null; then
              echo "DynamoDB Local is ready!"
              exit 0
            fi
            echo "Attempt $i: DynamoDB Local not ready yet, waiting..."
            sleep 2
          done
          echo "DynamoDB Local failed to start"
          docker ps -a
          docker logs $(docker ps -aq --filter ancestor=amazon/dynamodb-local:latest)
          exit 1

      - name: Install backend dependencies
        run: |
          cd backend
          pip install -r requirements.txt
          pip install pytest pytest-asyncio httpx black flake8 isort

      - name: Create DynamoDB test table
        run: |
          python3 << 'EOF'
          import boto3
          
          # Create DynamoDB client
          dynamodb = boto3.client(
              'dynamodb',
              endpoint_url='http://localhost:8000',
              region_name='us-east-1',
              aws_access_key_id='dummy',
              aws_secret_access_key='dummy'
          )
          
          # Create table
          try:
              dynamodb.create_table(
                  TableName='shoreexplorer_test',
                  KeySchema=[
                      {'AttributeName': 'PK', 'KeyType': 'HASH'},
                      {'AttributeName': 'SK', 'KeyType': 'RANGE'}
                  ],
                  AttributeDefinitions=[
                      {'AttributeName': 'PK', 'AttributeType': 'S'},
                      {'AttributeName': 'SK', 'AttributeType': 'S'},
                      {'AttributeName': 'GSI1PK', 'AttributeType': 'S'},
                      {'AttributeName': 'GSI1SK', 'AttributeType': 'S'}
                  ],
                  GlobalSecondaryIndexes=[
                      {
                          'IndexName': 'GSI1',
                          'KeySchema': [
                              {'AttributeName': 'GSI1PK', 'KeyType': 'HASH'},
                              {'AttributeName': 'GSI1SK', 'KeyType': 'RANGE'}
                          ],
                          'Projection': {'ProjectionType': 'ALL'},
                          'ProvisionedThroughput': {
                              'ReadCapacityUnits': 5,
                              'WriteCapacityUnits': 5
                          }
                      }
                  ],
                  BillingMode='PROVISIONED',
                  ProvisionedThroughput={
                      'ReadCapacityUnits': 5,
                      'WriteCapacityUnits': 5
                  }
              )
              print("Table 'shoreexplorer_test' created successfully!")
          except Exception as e:
              print(f"Error creating table: {e}")
              raise
          EOF

      - name: Backend lint
        run: cd backend && black --check . && isort --check-only . && flake8 .

      - name: Backend tests
        run: |
          cd backend
          pytest tests/ -v --tb=short
        env:
          DYNAMODB_TABLE_NAME: shoreexplorer_test
          AWS_DEFAULT_REGION: us-east-1
          DYNAMODB_ENDPOINT_URL: http://localhost:8000
          AWS_ACCESS_KEY_ID: dummy
          AWS_SECRET_ACCESS_KEY: dummy
          GROQ_API_KEY: test-key-not-real

      - uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: yarn
          cache-dependency-path: frontend/yarn.lock

      - name: Frontend tests
        run: |
          cd frontend
          yarn install --frozen-lockfile
          yarn test --watchAll=false --ci
        env:
          CI: true

  # ─────────────────────────────────────────────────
  # Deploy to production (requires approval)
  # ─────────────────────────────────────────────────
  deploy-prod:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: ci-check
    environment: production  # Requires manual approval if configured

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: ecr-login
        uses: aws-actions/amazon-ecr-login@v2

      - name: Set image variables
        id: vars
        env:
          GITHUB_SHA: ${{ github.sha }}
          INPUT_IMAGE_TAG: ${{ inputs.image_tag }}
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          GIT_SHA=$(echo "$GITHUB_SHA" | cut -c1-7)

          # Use provided tag or generate one
          if [[ -n "$INPUT_IMAGE_TAG" ]]; then
            IMAGE_TAG="$INPUT_IMAGE_TAG"
          else
            IMAGE_TAG="${ENVIRONMENT}-${GIT_SHA}"
          fi

          echo "ecr_base=${ECR_BASE}" >> "$GITHUB_OUTPUT"
          echo "image_tag=${IMAGE_TAG}" >> "$GITHUB_OUTPUT"
          echo "backend_image=${ECR_BASE}/${BACKEND_ECR_REPO}" >> "$GITHUB_OUTPUT"
          echo "frontend_image=${ECR_BASE}/${FRONTEND_ECR_REPO}" >> "$GITHUB_OUTPUT"

      - name: Get ALB DNS
        id: alb
        env:
          CUSTOM_DOMAIN: ${{ secrets.PROD_DOMAIN || '' }}
        run: |
          ALB_DNS=$(aws elbv2 describe-load-balancers \
            --names "${PROJECT_NAME}-${ENVIRONMENT}-alb" \
            --query "LoadBalancers[0].DNSName" \
            --output text --region "$AWS_REGION" 2>/dev/null || echo "")

          # Use custom domain if configured, otherwise use ALB DNS
          if [[ -n "$CUSTOM_DOMAIN" ]]; then
            # Production environment uses apex domain (no subdomain)
            BACKEND_URL="https://${CUSTOM_DOMAIN}"
            echo "Using custom domain: ${CUSTOM_DOMAIN}"
          elif [[ -n "$ALB_DNS" && "$ALB_DNS" != "None" ]]; then
            BACKEND_URL="https://${ALB_DNS}"
            echo "Using ALB DNS: ${ALB_DNS}"
          else
            BACKEND_URL="http://localhost:8001"
            echo "Using localhost fallback"
          fi

          echo "dns=${ALB_DNS}" >> "$GITHUB_OUTPUT"
          echo "backend_url=${BACKEND_URL}" >> "$GITHUB_OUTPUT"

      # ── Build and push backend ──────────────────────────────────
      - name: Build backend image
        run: |
          docker build \
            -t "${{ steps.vars.outputs.backend_image }}:${{ steps.vars.outputs.image_tag }}" \
            -t "${{ steps.vars.outputs.backend_image }}:${ENVIRONMENT}-latest" \
            -f backend/Dockerfile \
            backend/

      - name: Push backend image
        run: |
          docker push "${{ steps.vars.outputs.backend_image }}:${{ steps.vars.outputs.image_tag }}"
          docker push "${{ steps.vars.outputs.backend_image }}:${ENVIRONMENT}-latest"

      # ── Build and push frontend ────────────────────────────────
      - name: Build frontend image
        run: |
          docker build \
            --build-arg REACT_APP_BACKEND_URL="${{ steps.alb.outputs.backend_url }}" \
            -t "${{ steps.vars.outputs.frontend_image }}:${{ steps.vars.outputs.image_tag }}" \
            -t "${{ steps.vars.outputs.frontend_image }}:${ENVIRONMENT}-latest" \
            -f frontend/Dockerfile \
            frontend/

      - name: Push frontend image
        run: |
          docker push "${{ steps.vars.outputs.frontend_image }}:${{ steps.vars.outputs.image_tag }}"
          docker push "${{ steps.vars.outputs.frontend_image }}:${ENVIRONMENT}-latest"

      # ── Deploy to ECS (rolling deployment) ─────────────────────
      # First, register updated task definitions with correct env vars
      - name: Get secrets ARN
        id: secrets
        run: |
          SECRET_ARN=$(aws secretsmanager describe-secret \
            --secret-id "${PROJECT_NAME}-${ENVIRONMENT}-secrets" \
            --query 'ARN' --output text --region "$AWS_REGION" 2>/dev/null || echo "")
          
          if [[ -z "$SECRET_ARN" ]]; then
            echo "Warning: Could not find secrets ARN, skipping task definition update"
            echo "secret_arn=" >> "$GITHUB_OUTPUT"
          else
            echo "secret_arn=${SECRET_ARN}" >> "$GITHUB_OUTPUT"
          fi

      - name: Register backend task definition
        if: steps.secrets.outputs.secret_arn != ''
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          SECRET_ARN="${{ steps.secrets.outputs.secret_arn }}"
          
          # Get IAM role ARNs
          EXECUTION_ROLE_ARN=$(aws iam get-role \
            --role-name "${PROJECT_NAME}-${ENVIRONMENT}-ecs-task-execution-role" \
            --query 'Role.Arn' --output text --region "$AWS_REGION")
          TASK_ROLE_ARN=$(aws iam get-role \
            --role-name "${PROJECT_NAME}-${ENVIRONMENT}-ecs-task-role" \
            --query 'Role.Arn' --output text --region "$AWS_REGION")
          
          # Register backend task definition with GROQ_API_KEY
          aws ecs register-task-definition \
            --family "${PROJECT_NAME}-${ENVIRONMENT}-backend-task" \
            --network-mode awsvpc \
            --requires-compatibilities FARGATE \
            --cpu 256 \
            --memory 512 \
            --execution-role-arn "$EXECUTION_ROLE_ARN" \
            --task-role-arn "$TASK_ROLE_ARN" \
            --container-definitions "[{
              \"name\": \"backend\",
              \"image\": \"${ECR_BASE}/${BACKEND_ECR_REPO}:latest\",
              \"essential\": true,
              \"portMappings\": [{\"containerPort\": 8001, \"protocol\": \"tcp\"}],
              \"secrets\": [
                {\"name\": \"GROQ_API_KEY\", \"valueFrom\": \"${SECRET_ARN}:GROQ_API_KEY::\"}
              ],
              \"environment\": [
                {\"name\": \"DYNAMODB_TABLE_NAME\", \"value\": \"${PROJECT_NAME}-${ENVIRONMENT}\"},
                {\"name\": \"AWS_DEFAULT_REGION\", \"value\": \"${AWS_REGION}\"}
              ],
              \"logConfiguration\": {
                \"logDriver\": \"awslogs\",
                \"options\": {
                  \"awslogs-group\": \"/ecs/${PROJECT_NAME}-${ENVIRONMENT}-backend\",
                  \"awslogs-region\": \"${AWS_REGION}\",
                  \"awslogs-stream-prefix\": \"ecs\"
                }
              },
              \"healthCheck\": {
                \"command\": [\"CMD-SHELL\", \"python -c \\\"import urllib.request; exit(0 if urllib.request.urlopen('http://localhost:8001/api/health').status == 200 else 1)\\\"\"],
                \"interval\": 30,
                \"timeout\": 10,
                \"retries\": 3,
                \"startPeriod\": 60
              }
            }]" \
            --tags "key=Project,value=${PROJECT_NAME}" "key=Environment,value=${ENVIRONMENT}" \
            --region "$AWS_REGION"

      - name: Register frontend task definition
        if: steps.secrets.outputs.secret_arn != ''
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          
          # Get IAM role ARNs
          EXECUTION_ROLE_ARN=$(aws iam get-role \
            --role-name "${PROJECT_NAME}-${ENVIRONMENT}-ecs-task-execution-role" \
            --query 'Role.Arn' --output text --region "$AWS_REGION")
          TASK_ROLE_ARN=$(aws iam get-role \
            --role-name "${PROJECT_NAME}-${ENVIRONMENT}-ecs-task-role" \
            --query 'Role.Arn' --output text --region "$AWS_REGION")
          
          # Register frontend task definition
          aws ecs register-task-definition \
            --family "${PROJECT_NAME}-${ENVIRONMENT}-frontend-task" \
            --network-mode awsvpc \
            --requires-compatibilities FARGATE \
            --cpu 256 \
            --memory 512 \
            --execution-role-arn "$EXECUTION_ROLE_ARN" \
            --task-role-arn "$TASK_ROLE_ARN" \
            --container-definitions "[{
              \"name\": \"frontend\",
              \"image\": \"${ECR_BASE}/${FRONTEND_ECR_REPO}:latest\",
              \"essential\": true,
              \"portMappings\": [{\"containerPort\": 8080, \"protocol\": \"tcp\"}],
              \"logConfiguration\": {
                \"logDriver\": \"awslogs\",
                \"options\": {
                  \"awslogs-group\": \"/ecs/${PROJECT_NAME}-${ENVIRONMENT}-frontend\",
                  \"awslogs-region\": \"${AWS_REGION}\",
                  \"awslogs-stream-prefix\": \"ecs\"
                }
              },
              \"healthCheck\": {
                \"command\": [\"CMD-SHELL\", \"wget --quiet --tries=1 --spider http://localhost:8080/ || exit 1\"],
                \"interval\": 30,
                \"timeout\": 5,
                \"retries\": 3,
                \"startPeriod\": 30
              }
            }]" \
            --tags "key=Project,value=${PROJECT_NAME}" "key=Environment,value=${ENVIRONMENT}" \
            --region "$AWS_REGION"

      - name: Update backend ECS service
        run: |
          aws ecs update-service \
            --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
            --service "${PROJECT_NAME}-${ENVIRONMENT}-backend" \
            --task-definition "${PROJECT_NAME}-${ENVIRONMENT}-backend-task" \
            --force-new-deployment \
            --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100" \
            --region "$AWS_REGION"

      - name: Update frontend ECS service
        run: |
          aws ecs update-service \
            --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
            --service "${PROJECT_NAME}-${ENVIRONMENT}-frontend" \
            --task-definition "${PROJECT_NAME}-${ENVIRONMENT}-frontend-task" \
            --force-new-deployment \
            --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100" \
            --region "$AWS_REGION"

      - name: Wait for deployment to stabilize
        run: |
          # aws ecs wait services-stable uses `length(deployments) == 1` which fails
          # when ECS keeps old INACTIVE deployment entries after a successful rolling
          # update (circuit-breaker or standard). We poll manually, ignoring INACTIVE
          # entries, to match what the ECS console considers "stable".
          echo "Waiting for production services to stabilize (up to 10 minutes)..."
          CLUSTER="${PROJECT_NAME}-${ENVIRONMENT}-cluster"
          SERVICES=(
            "${PROJECT_NAME}-${ENVIRONMENT}-backend"
            "${PROJECT_NAME}-${ENVIRONMENT}-frontend"
          )
          MAX_WAIT=600
          POLL_INTERVAL=15
          elapsed=0

          while [[ $elapsed -lt $MAX_WAIT ]]; do
            all_stable=true
            for svc in "${SERVICES[@]}"; do
              svc_info=$(aws ecs describe-services \
                --cluster "$CLUSTER" --services "$svc" --region "$AWS_REGION" \
                --query "services[0].{active: length(deployments[?status!='INACTIVE']), running: runningCount, desired: desiredCount}" \
                --output json)
              active=$(echo "$svc_info" | jq -r '.active')
              running=$(echo "$svc_info" | jq -r '.running')
              desired=$(echo "$svc_info" | jq -r '.desired')
              echo "${svc}: active_deployments=${active}, running=${running}, desired=${desired}"
              if [[ "$active" != "1" || "$running" != "$desired" ]]; then
                all_stable=false
              fi
            done

            if $all_stable; then
              echo "✅ All services stable!"
              break
            fi

            sleep $POLL_INTERVAL
            elapsed=$((elapsed + POLL_INTERVAL))
          done

          if [[ $elapsed -ge $MAX_WAIT ]]; then
            echo "❌ Timed out after ${MAX_WAIT}s waiting for services to stabilize"
            exit 1
          fi
          echo "Production deployment complete!"

      # ── Smoke test ─────────────────────────────────────────────
      - name: Run production smoke test
        if: steps.alb.outputs.backend_url != 'http://localhost:8001'
        env:
          BACKEND_URL: ${{ steps.alb.outputs.backend_url }}
        run: |
          echo "Running smoke tests against ${BACKEND_URL}"
          sleep 30

          # Test health endpoint
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            "${BACKEND_URL}/api/health" \
            --max-time 30 --retry 3 --retry-delay 10 || echo "000")

          if [[ "$HTTP_CODE" == "200" ]]; then
            echo "✅ Health check passed"
          else
            echo "❌ Health check failed (HTTP $HTTP_CODE)"
            exit 1
          fi

          # Test frontend (use same base URL without /api)
          FRONTEND_URL="${BACKEND_URL%/api*}"
          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            "${FRONTEND_URL}/" \
            --max-time 30 --retry 3 --retry-delay 10 || echo "000")

          if [[ "$HTTP_CODE" == "200" ]]; then
            echo "✅ Frontend accessible"
          else
            echo "❌ Frontend not accessible (HTTP $HTTP_CODE)"
            exit 1
          fi

      - name: Deployment summary
        if: always()
        env:
          BACKEND_URL: ${{ steps.alb.outputs.backend_url }}
        run: |
          echo "## Production Deployment Summary" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "| Item | Value |" >> "$GITHUB_STEP_SUMMARY"
          echo "|------|-------|" >> "$GITHUB_STEP_SUMMARY"
          echo "| Environment | \`production\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Image Tag | \`${{ steps.vars.outputs.image_tag }}\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Commit | \`${{ github.sha }}\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Tag | \`${{ github.ref_name }}\` |" >> "$GITHUB_STEP_SUMMARY"
          if [[ "${BACKEND_URL}" != "http://localhost:8001" ]]; then
            echo "| App URL | ${BACKEND_URL} |" >> "$GITHUB_STEP_SUMMARY"
            echo "| API Health | ${BACKEND_URL}/api/health |" >> "$GITHUB_STEP_SUMMARY"
          fi

  # ─────────────────────────────────────────────────
  # Rollback job (manual trigger only)
  # ─────────────────────────────────────────────────
  rollback:
    name: Rollback Production
    runs-on: ubuntu-latest
    if: failure() && needs.deploy-prod.result == 'failure'
    needs: deploy-prod
    environment: production

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Rollback to previous task definition
        run: |
          echo "⚠️  Deployment failed. Rolling back..."

          for SERVICE in backend frontend; do
            FULL_SERVICE="${PROJECT_NAME}-${ENVIRONMENT}-${SERVICE}"
            FAMILY="${PROJECT_NAME}-${ENVIRONMENT}-${SERVICE}-task"

            # Get the previous task definition (second most recent)
            PREVIOUS_TD=$(aws ecs list-task-definitions \
              --family-prefix "$FAMILY" \
              --sort DESC \
              --query "taskDefinitionArns[1]" \
              --output text --region "$AWS_REGION")

            if [[ -n "$PREVIOUS_TD" && "$PREVIOUS_TD" != "None" ]]; then
              aws ecs update-service \
                --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
                --service "$FULL_SERVICE" \
                --task-definition "$PREVIOUS_TD" \
                --force-new-deployment \
                --region "$AWS_REGION"
              echo "Rolled back $SERVICE to: $PREVIOUS_TD"
            else
              echo "No previous task definition found for $SERVICE"
            fi
          done

      - name: Wait for rollback to stabilize
        run: |
          aws ecs wait services-stable \
            --cluster "${PROJECT_NAME}-${ENVIRONMENT}-cluster" \
            --services "${PROJECT_NAME}-${ENVIRONMENT}-backend" "${PROJECT_NAME}-${ENVIRONMENT}-frontend" \
            --region "$AWS_REGION"
          echo "Rollback complete."
