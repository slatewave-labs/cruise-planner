name: Setup & Deploy Production Environment

# Provisions all AWS infrastructure from scratch and deploys the application.
# Self-healing: skips resources that already exist.

on:
  workflow_dispatch: {}

# Prevent parallel setup runs on the same environment
concurrency:
  group: setup-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}
  ENVIRONMENT: prod
  PROJECT_NAME: shoreexplorer
  BACKEND_ECR_REPO: shoreexplorer-backend
  FRONTEND_ECR_REPO: shoreexplorer-frontend

jobs:
  setup-and-deploy-prod:
    name: Setup & Deploy Production
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Set variables
        id: vars
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_BASE="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
          GIT_SHA=$(echo "${{ github.sha }}" | cut -c1-7)
          IMAGE_TAG="${ENVIRONMENT}-${GIT_SHA}"

          echo "account_id=${ACCOUNT_ID}" >> "$GITHUB_OUTPUT"
          echo "ecr_base=${ECR_BASE}" >> "$GITHUB_OUTPUT"
          echo "image_tag=${IMAGE_TAG}" >> "$GITHUB_OUTPUT"

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      #  INFRASTRUCTURE PROVISIONING
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      # â”€â”€ 1. ECR Repositories (shared) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Create ECR repositories
        run: |
          echo "ğŸ“¦ Ensuring ECR repositories exist..."
          for REPO in "$BACKEND_ECR_REPO" "$FRONTEND_ECR_REPO"; do
            if aws ecr describe-repositories --repository-names "$REPO" --region "$AWS_REGION" &>/dev/null; then
              echo "  â­ï¸  ECR repo exists: $REPO"
            else
              aws ecr create-repository --repository-name "$REPO" --region "$AWS_REGION" \
                --image-scanning-configuration scanOnPush=true --image-tag-mutability MUTABLE \
                --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value=shared \
                --no-cli-pager
              echo "  âœ… Created ECR repo: $REPO"
            fi

            # Set lifecycle policy
            aws ecr put-lifecycle-policy --repository-name "$REPO" --region "$AWS_REGION" \
              --lifecycle-policy-text '{"rules":[{"rulePriority":1,"description":"Keep last 10","selection":{"tagStatus":"any","countType":"imageCountMoreThan","countNumber":10},"action":{"type":"expire"}}]}' \
              --no-cli-pager >/dev/null 2>&1 || true
          done

      # â”€â”€ 2. VPC & Networking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Create networking
        id: networking
        run: |
          echo "ğŸŒ Provisioning networking..."
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"
          # Prod uses different CIDR range to avoid conflicts
          VPC_CIDR="10.1.0.0/16"
          AZ1="${AWS_REGION}a"
          AZ2="${AWS_REGION}b"

          # VPC
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${APP_NAME}-vpc" \
            --query "Vpcs[0].VpcId" --output text --region "$AWS_REGION" 2>/dev/null || echo "None")

          if [[ "$VPC_ID" == "None" || -z "$VPC_ID" ]]; then
            VPC_ID=$(aws ec2 create-vpc --cidr-block "$VPC_CIDR" \
              --tag-specifications "ResourceType=vpc,Tags=[{Key=Name,Value=${APP_NAME}-vpc},{Key=Project,Value=$PROJECT_NAME},{Key=Environment,Value=$ENVIRONMENT}]" \
              --query "Vpc.VpcId" --output text --region "$AWS_REGION")
            aws ec2 modify-vpc-attribute --vpc-id "$VPC_ID" --enable-dns-hostnames '{"Value":true}' --region "$AWS_REGION"
            aws ec2 modify-vpc-attribute --vpc-id "$VPC_ID" --enable-dns-support '{"Value":true}' --region "$AWS_REGION"
            echo "  âœ… Created VPC: $VPC_ID"
          else
            echo "  â­ï¸  VPC exists: $VPC_ID"
          fi

          # Internet Gateway
          IGW_ID=$(aws ec2 describe-internet-gateways \
            --filters "Name=tag:Name,Values=${APP_NAME}-igw" \
            --query "InternetGateways[0].InternetGatewayId" --output text --region "$AWS_REGION" 2>/dev/null || echo "None")

          if [[ "$IGW_ID" == "None" || -z "$IGW_ID" ]]; then
            IGW_ID=$(aws ec2 create-internet-gateway \
              --tag-specifications "ResourceType=internet-gateway,Tags=[{Key=Name,Value=${APP_NAME}-igw},{Key=Project,Value=$PROJECT_NAME},{Key=Environment,Value=$ENVIRONMENT}]" \
              --query "InternetGateway.InternetGatewayId" --output text --region "$AWS_REGION")
            aws ec2 attach-internet-gateway --internet-gateway-id "$IGW_ID" --vpc-id "$VPC_ID" --region "$AWS_REGION"
            echo "  âœ… Created IGW: $IGW_ID"
          else
            echo "  â­ï¸  IGW exists: $IGW_ID"
          fi

          # Helper function for subnet creation
          create_subnet() {
            local NAME="$1" CIDR="$2" AZ="$3" MAP_PUBLIC="$4"
            local SID
            SID=$(aws ec2 describe-subnets \
              --filters "Name=tag:Name,Values=$NAME" "Name=vpc-id,Values=$VPC_ID" \
              --query "Subnets[0].SubnetId" --output text --region "$AWS_REGION" 2>/dev/null || echo "None")
            if [[ "$SID" == "None" || -z "$SID" ]]; then
              SID=$(aws ec2 create-subnet --vpc-id "$VPC_ID" --cidr-block "$CIDR" --availability-zone "$AZ" \
                --tag-specifications "ResourceType=subnet,Tags=[{Key=Name,Value=$NAME},{Key=Project,Value=$PROJECT_NAME},{Key=Environment,Value=$ENVIRONMENT}]" \
                --query "Subnet.SubnetId" --output text --region "$AWS_REGION")
              if [[ "$MAP_PUBLIC" == "true" ]]; then
                aws ec2 modify-subnet-attribute --subnet-id "$SID" --map-public-ip-on-launch --region "$AWS_REGION"
              fi
              echo "  âœ… Created subnet: $NAME ($SID)" >&2
            else
              echo "  â­ï¸  Subnet exists: $NAME ($SID)" >&2
            fi
            echo "$SID"
          }

          PUB1=$(create_subnet "${APP_NAME}-public-1" "10.1.1.0/24" "$AZ1" "true")
          PUB2=$(create_subnet "${APP_NAME}-public-2" "10.1.2.0/24" "$AZ2" "true")
          create_subnet "${APP_NAME}-private-1" "10.1.3.0/24" "$AZ1" "false" >/dev/null
          create_subnet "${APP_NAME}-private-2" "10.1.4.0/24" "$AZ2" "false" >/dev/null

          # Route Table
          RT_ID=$(aws ec2 describe-route-tables \
            --filters "Name=tag:Name,Values=${APP_NAME}-public-rt" "Name=vpc-id,Values=$VPC_ID" \
            --query "RouteTables[0].RouteTableId" --output text --region "$AWS_REGION" 2>/dev/null || echo "None")

          if [[ "$RT_ID" == "None" || -z "$RT_ID" ]]; then
            RT_ID=$(aws ec2 create-route-table --vpc-id "$VPC_ID" \
              --tag-specifications "ResourceType=route-table,Tags=[{Key=Name,Value=${APP_NAME}-public-rt},{Key=Project,Value=$PROJECT_NAME},{Key=Environment,Value=$ENVIRONMENT}]" \
              --query "RouteTable.RouteTableId" --output text --region "$AWS_REGION")
            aws ec2 create-route --route-table-id "$RT_ID" --destination-cidr-block "0.0.0.0/0" --gateway-id "$IGW_ID" --region "$AWS_REGION" >/dev/null
            echo "  âœ… Created route table: $RT_ID"
          else
            echo "  â­ï¸  Route table exists: $RT_ID"
          fi

          # Associate subnets
          for SID in "$PUB1" "$PUB2"; do
            ASSOC=$(aws ec2 describe-route-tables \
              --filters "Name=association.subnet-id,Values=$SID" "Name=route-table-id,Values=$RT_ID" \
              --query "RouteTables[0].RouteTableId" --output text --region "$AWS_REGION" 2>/dev/null || echo "None")
            if [[ "$ASSOC" == "None" || -z "$ASSOC" ]]; then
              aws ec2 associate-route-table --subnet-id "$SID" --route-table-id "$RT_ID" --region "$AWS_REGION" >/dev/null
            fi
          done

          # Security Groups
          create_sg() {
            local SG_NAME="$1" DESC="$2"
            local SGID
            SGID=$(aws ec2 describe-security-groups \
              --filters "Name=group-name,Values=$SG_NAME" "Name=vpc-id,Values=$VPC_ID" \
              --query "SecurityGroups[0].GroupId" --output text --region "$AWS_REGION" 2>/dev/null || echo "None")
            if [[ "$SGID" == "None" || -z "$SGID" ]]; then
              SGID=$(aws ec2 create-security-group --group-name "$SG_NAME" --description "$DESC" --vpc-id "$VPC_ID" \
                --tag-specifications "ResourceType=security-group,Tags=[{Key=Name,Value=$SG_NAME},{Key=Project,Value=$PROJECT_NAME},{Key=Environment,Value=$ENVIRONMENT}]" \
                --query "GroupId" --output text --region "$AWS_REGION")
              echo "  âœ… Created SG: $SG_NAME ($SGID)" >&2
            else
              echo "  â­ï¸  SG exists: $SG_NAME ($SGID)" >&2
            fi
            echo "$SGID"
          }

          ALB_SG_ID=$(create_sg "${APP_NAME}-alb-sg" "ALB security group")
          ECS_SG_ID=$(create_sg "${APP_NAME}-ecs-sg" "ECS tasks security group")

          # SG rules (idempotent)
          for PORT in 80 443; do
            aws ec2 authorize-security-group-ingress --group-id "$ALB_SG_ID" \
              --protocol tcp --port "$PORT" --cidr "0.0.0.0/0" --region "$AWS_REGION" 2>/dev/null || true
          done
          aws ec2 authorize-security-group-ingress --group-id "$ECS_SG_ID" \
            --protocol tcp --port 8001 --source-group "$ALB_SG_ID" --region "$AWS_REGION" 2>/dev/null || true
          aws ec2 authorize-security-group-ingress --group-id "$ECS_SG_ID" \
            --protocol tcp --port 8080 --source-group "$ALB_SG_ID" --region "$AWS_REGION" 2>/dev/null || true

          # Outputs
          echo "vpc_id=${VPC_ID}" >> "$GITHUB_OUTPUT"
          echo "pub_subnet_1=${PUB1}" >> "$GITHUB_OUTPUT"
          echo "pub_subnet_2=${PUB2}" >> "$GITHUB_OUTPUT"
          echo "alb_sg_id=${ALB_SG_ID}" >> "$GITHUB_OUTPUT"
          echo "ecs_sg_id=${ECS_SG_ID}" >> "$GITHUB_OUTPUT"

      # â”€â”€ 3. Secrets Manager â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Create secrets
        id: secrets
        env:
          GROQ_API_KEY: ${{ secrets.PROD_GROQ_API_KEY }}
        run: |
          echo "ğŸ” Ensuring secrets exist..."
          SECRET_NAME="${PROJECT_NAME}-${ENVIRONMENT}-secrets"

          if aws secretsmanager describe-secret --secret-id "$SECRET_NAME" --region "$AWS_REGION" &>/dev/null; then
            echo "  â­ï¸  Secret exists: $SECRET_NAME"
            # Update if GROQ_API_KEY is provided
            if [[ -n "${GROQ_API_KEY:-}" ]]; then
              aws secretsmanager update-secret --secret-id "$SECRET_NAME" \
                --secret-string "{\"GROQ_API_KEY\":\"${GROQ_API_KEY}\"}" \
                --region "$AWS_REGION" --no-cli-pager >/dev/null 2>&1 || true
            fi
          else
            aws secretsmanager create-secret --name "$SECRET_NAME" \
              --description "ShoreExplorer ${ENVIRONMENT} environment secrets" \
              --secret-string "{\"GROQ_API_KEY\":\"${GROQ_API_KEY:-placeholder}\"}" \
              --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "  âœ… Created secret: $SECRET_NAME"
          fi

          SECRET_ARN=$(aws secretsmanager describe-secret --secret-id "$SECRET_NAME" \
            --query "ARN" --output text --region "$AWS_REGION")
          echo "secret_arn=${SECRET_ARN}" >> "$GITHUB_OUTPUT"

      # â”€â”€ 4. IAM Roles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Create IAM roles
        run: |
          echo "ğŸ”‘ Ensuring IAM roles exist..."
          ACCOUNT_ID="${{ steps.vars.outputs.account_id }}"
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"
          SECRET_ARN="${{ steps.secrets.outputs.secret_arn }}"

          TRUST_POLICY='{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"ecs-tasks.amazonaws.com"},"Action":"sts:AssumeRole"}]}'

          # Execution role
          EXEC_ROLE="${APP_NAME}-ecs-task-execution-role"
          if aws iam get-role --role-name "$EXEC_ROLE" &>/dev/null; then
            echo "  â­ï¸  Role exists: $EXEC_ROLE"
          else
            aws iam create-role --role-name "$EXEC_ROLE" --assume-role-policy-document "$TRUST_POLICY" \
              --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" --no-cli-pager >/dev/null
            aws iam attach-role-policy --role-name "$EXEC_ROLE" \
              --policy-arn "arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy"
            echo "  âœ… Created role: $EXEC_ROLE"
          fi

          # Execution role inline policies (idempotent put)
          cat > /tmp/exec-policy.json <<POLICY
          {"Version":"2012-10-17","Statement":[
            {"Effect":"Allow","Action":["secretsmanager:GetSecretValue"],"Resource":["arn:aws:secretsmanager:${AWS_REGION}:${ACCOUNT_ID}:secret:${APP_NAME}-secrets*"]},
            {"Effect":"Allow","Action":["logs:CreateLogStream","logs:PutLogEvents","logs:CreateLogGroup"],"Resource":"arn:aws:logs:${AWS_REGION}:${ACCOUNT_ID}:*"}
          ]}
          POLICY
          aws iam put-role-policy --role-name "$EXEC_ROLE" --policy-name "${APP_NAME}-secrets-and-logs" \
            --policy-document file:///tmp/exec-policy.json 2>/dev/null || true

          # Task role
          TASK_ROLE="${APP_NAME}-ecs-task-role"
          if aws iam get-role --role-name "$TASK_ROLE" &>/dev/null; then
            echo "  â­ï¸  Role exists: $TASK_ROLE"
          else
            aws iam create-role --role-name "$TASK_ROLE" --assume-role-policy-document "$TRUST_POLICY" \
              --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" --no-cli-pager >/dev/null
            echo "  âœ… Created role: $TASK_ROLE"
          fi

          # Task role inline policies (idempotent put)
          cat > /tmp/task-policy.json <<POLICY
          {"Version":"2012-10-17","Statement":[
            {"Effect":"Allow","Action":["dynamodb:GetItem","dynamodb:PutItem","dynamodb:UpdateItem","dynamodb:DeleteItem","dynamodb:Query","dynamodb:Scan","dynamodb:DescribeTable"],"Resource":["arn:aws:dynamodb:${AWS_REGION}:${ACCOUNT_ID}:table/${PROJECT_NAME}-${ENVIRONMENT}","arn:aws:dynamodb:${AWS_REGION}:${ACCOUNT_ID}:table/${PROJECT_NAME}-${ENVIRONMENT}/index/*"]},
            {"Sid":"CloudWatchMetrics","Effect":"Allow","Action":["cloudwatch:PutMetricData"],"Resource":"*","Condition":{"StringEquals":{"cloudwatch:namespace":"${PROJECT_NAME}/${ENVIRONMENT}"}}}
          ]}
          POLICY
          aws iam put-role-policy --role-name "$TASK_ROLE" --policy-name "${APP_NAME}-dynamodb-access" \
            --policy-document file:///tmp/task-policy.json 2>/dev/null || true

      # â”€â”€ 5. DynamoDB Table â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Create DynamoDB table
        run: |
          echo "ğŸ—„ï¸  Ensuring DynamoDB table exists..."
          TABLE_NAME="${PROJECT_NAME}-${ENVIRONMENT}"

          if aws dynamodb describe-table --table-name "$TABLE_NAME" --region "$AWS_REGION" &>/dev/null; then
            echo "  â­ï¸  Table exists: $TABLE_NAME"
          else
            aws dynamodb create-table --table-name "$TABLE_NAME" \
              --attribute-definitions \
                AttributeName=PK,AttributeType=S \
                AttributeName=SK,AttributeType=S \
                AttributeName=GSI1PK,AttributeType=S \
                AttributeName=GSI1SK,AttributeType=S \
              --key-schema AttributeName=PK,KeyType=HASH AttributeName=SK,KeyType=RANGE \
              --billing-mode PAY_PER_REQUEST \
              --global-secondary-indexes '[{"IndexName":"GSI1","KeySchema":[{"AttributeName":"GSI1PK","KeyType":"HASH"},{"AttributeName":"GSI1SK","KeyType":"RANGE"}],"Projection":{"ProjectionType":"ALL"}}]' \
              --tags Key=Environment,Value="$ENVIRONMENT" Key=Application,Value=ShoreExplorer Key=ManagedBy,Value=GitHubActions \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "  â³ Waiting for table to become active..."
            aws dynamodb wait table-exists --table-name "$TABLE_NAME" --region "$AWS_REGION"
            echo "  âœ… Created table: $TABLE_NAME"

            # Enable point-in-time recovery for production
            aws dynamodb update-continuous-backups --table-name "$TABLE_NAME" \
              --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true \
              --region "$AWS_REGION" --no-cli-pager >/dev/null 2>&1 || true
            echo "  âœ… Enabled point-in-time recovery"
          fi

      # â”€â”€ 6. ECS Cluster & Log Groups â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Create ECS cluster and log groups
        run: |
          echo "ğŸš€ Ensuring ECS cluster and log groups exist..."
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"
          CLUSTER="${APP_NAME}-cluster"

          # Log groups
          for LG in "/ecs/${APP_NAME}-backend" "/ecs/${APP_NAME}-frontend"; do
            if aws logs describe-log-groups --log-group-name-prefix "$LG" --region "$AWS_REGION" \
              --query "logGroups[?logGroupName=='$LG']" --output text | grep -q "$LG"; then
              echo "  â­ï¸  Log group exists: $LG"
            else
              aws logs create-log-group --log-group-name "$LG" --region "$AWS_REGION" \
                --tags Project="$PROJECT_NAME",Environment="$ENVIRONMENT"
              echo "  âœ… Created log group: $LG"
            fi
            aws logs put-retention-policy --log-group-name "$LG" --retention-in-days 14 --region "$AWS_REGION" 2>/dev/null || true
          done

          # Cluster
          CLUSTER_STATUS=$(aws ecs describe-clusters --clusters "$CLUSTER" --region "$AWS_REGION" \
            --query "clusters[0].status" --output text 2>/dev/null || echo "MISSING")
          if [[ "$CLUSTER_STATUS" == "ACTIVE" ]]; then
            echo "  â­ï¸  Cluster exists: $CLUSTER"
          else
            aws ecs create-cluster --cluster-name "$CLUSTER" \
              --capacity-providers FARGATE \
              --default-capacity-provider-strategy capacityProvider=FARGATE,weight=1 \
              --settings '[{"name":"containerInsights","value":"disabled"}]' \
              --tags key=Project,value="$PROJECT_NAME" key=Environment,value="$ENVIRONMENT" \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "  âœ… Created cluster: $CLUSTER"
          fi

      # â”€â”€ 7. ALB & Target Groups â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Create ALB and target groups
        id: alb
        run: |
          echo "âš–ï¸  Ensuring ALB and target groups exist..."
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"
          ALB_NAME="${APP_NAME}-alb"
          VPC_ID="${{ steps.networking.outputs.vpc_id }}"
          PUB1="${{ steps.networking.outputs.pub_subnet_1 }}"
          PUB2="${{ steps.networking.outputs.pub_subnet_2 }}"
          ALB_SG="${{ steps.networking.outputs.alb_sg_id }}"

          # ALB
          ALB_ARN=$(aws elbv2 describe-load-balancers --names "$ALB_NAME" \
            --query "LoadBalancers[0].LoadBalancerArn" --output text \
            --region "$AWS_REGION" 2>/dev/null || echo "None")

          if [[ "$ALB_ARN" == "None" || -z "$ALB_ARN" ]]; then
            ALB_ARN=$(aws elbv2 create-load-balancer --name "$ALB_NAME" \
              --subnets "$PUB1" "$PUB2" --security-groups "$ALB_SG" \
              --scheme internet-facing --type application --ip-address-type ipv4 \
              --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" \
              --query "LoadBalancers[0].LoadBalancerArn" --output text --region "$AWS_REGION")
            echo "  âœ… Created ALB: $ALB_NAME"
          else
            echo "  â­ï¸  ALB exists: $ALB_NAME"
          fi

          ALB_DNS=$(aws elbv2 describe-load-balancers --load-balancer-arns "$ALB_ARN" \
            --query "LoadBalancers[0].DNSName" --output text --region "$AWS_REGION")

          # Target Groups
          create_tg() {
            local TG_NAME="$1" PORT="$2" HEALTH="$3"
            local TG_ARN
            TG_ARN=$(aws elbv2 describe-target-groups --names "$TG_NAME" \
              --query "TargetGroups[0].TargetGroupArn" --output text \
              --region "$AWS_REGION" 2>/dev/null || echo "None")
            if [[ "$TG_ARN" == "None" || -z "$TG_ARN" ]]; then
              TG_ARN=$(aws elbv2 create-target-group --name "$TG_NAME" \
                --protocol HTTP --port "$PORT" --vpc-id "$VPC_ID" --target-type ip \
                --health-check-protocol HTTP --health-check-path "$HEALTH" \
                --health-check-interval-seconds 30 --health-check-timeout-seconds 10 \
                --healthy-threshold-count 2 --unhealthy-threshold-count 3 --matcher HttpCode=200 \
                --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" \
                --query "TargetGroups[0].TargetGroupArn" --output text --region "$AWS_REGION")
              echo "  âœ… Created TG: $TG_NAME" >&2
            else
              echo "  â­ï¸  TG exists: $TG_NAME" >&2
            fi
            echo "$TG_ARN"
          }

          BACKEND_TG_ARN=$(create_tg "${APP_NAME}-backend-tg" 8001 "/api/health")
          FRONTEND_TG_ARN=$(create_tg "${APP_NAME}-frontend-tg" 8080 "/")

          # HTTP Listener (port 80)
          LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn "$ALB_ARN" \
            --query "Listeners[?Port==\`80\`].ListenerArn" --output text \
            --region "$AWS_REGION" 2>/dev/null || echo "")

          if [[ -z "$LISTENER_ARN" || "$LISTENER_ARN" == "None" ]]; then
            LISTENER_ARN=$(aws elbv2 create-listener --load-balancer-arn "$ALB_ARN" \
              --protocol HTTP --port 80 \
              --default-actions Type=forward,TargetGroupArn="$FRONTEND_TG_ARN" \
              --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" \
              --query "Listeners[0].ListenerArn" --output text --region "$AWS_REGION")
            echo "  âœ… Created HTTP listener"
          else
            echo "  â­ï¸  HTTP listener exists"
          fi

          # Listener rule: /api/* â†’ backend
          EXISTING_RULES=$(aws elbv2 describe-rules --listener-arn "$LISTENER_ARN" \
            --query "Rules[?Conditions[?Field=='path-pattern' && Values[0]=='/api/*']].RuleArn" \
            --output text --region "$AWS_REGION" 2>/dev/null || echo "")

          if [[ -z "$EXISTING_RULES" || "$EXISTING_RULES" == "None" ]]; then
            aws elbv2 create-rule --listener-arn "$LISTENER_ARN" --priority 10 \
              --conditions Field=path-pattern,Values='/api/*' \
              --actions Type=forward,TargetGroupArn="$BACKEND_TG_ARN" \
              --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "  âœ… Created /api/* listener rule"
          else
            echo "  â­ï¸  /api/* rule exists"
          fi

          echo "alb_arn=${ALB_ARN}" >> "$GITHUB_OUTPUT"
          echo "alb_dns=${ALB_DNS}" >> "$GITHUB_OUTPUT"
          echo "backend_tg_arn=${BACKEND_TG_ARN}" >> "$GITHUB_OUTPUT"
          echo "frontend_tg_arn=${FRONTEND_TG_ARN}" >> "$GITHUB_OUTPUT"
          echo "listener_arn=${LISTENER_ARN}" >> "$GITHUB_OUTPUT"

      # â”€â”€ 8. HTTPS Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Setup HTTPS
        id: https
        env:
          PROD_DOMAIN: ${{ secrets.PROD_DOMAIN || '' }}
        run: |
          echo "ğŸ”’ Setting up HTTPS..."
          ALB_ARN="${{ steps.alb.outputs.alb_arn }}"
          FRONTEND_TG_ARN="${{ steps.alb.outputs.frontend_tg_arn }}"
          BACKEND_TG_ARN="${{ steps.alb.outputs.backend_tg_arn }}"
          LISTENER_ARN="${{ steps.alb.outputs.listener_arn }}"

          if [[ -z "$PROD_DOMAIN" ]]; then
            echo "  â­ï¸  PROD_DOMAIN not set â€” skipping HTTPS"
            echo "backend_url=http://${{ steps.alb.outputs.alb_dns }}" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Prod uses apex domain (no subdomain)
          FULL_DOMAIN="${PROD_DOMAIN}"

          # Find existing wildcard certificate
          CERT_ARN=$(aws acm list-certificates --region "$AWS_REGION" \
            --query "CertificateSummaryList[?DomainName=='*.${PROD_DOMAIN}' && Status=='ISSUED'].CertificateArn" \
            --output text 2>/dev/null || echo "")

          if [[ -z "$CERT_ARN" || "$CERT_ARN" == "None" ]]; then
            echo "  âš ï¸  No issued wildcard certificate found for *.${PROD_DOMAIN}"
            echo "  Using ALB DNS directly"
            echo "backend_url=http://${{ steps.alb.outputs.alb_dns }}" >> "$GITHUB_OUTPUT"
            exit 0
          fi

          echo "  âœ… Found certificate: $CERT_ARN"

          # HTTPS listener (port 443)
          HTTPS_LISTENER=$(aws elbv2 describe-listeners --load-balancer-arn "$ALB_ARN" \
            --query "Listeners[?Port==\`443\`].ListenerArn" --output text \
            --region "$AWS_REGION" 2>/dev/null || echo "")

          if [[ -z "$HTTPS_LISTENER" || "$HTTPS_LISTENER" == "None" ]]; then
            HTTPS_LISTENER=$(aws elbv2 create-listener --load-balancer-arn "$ALB_ARN" \
              --protocol HTTPS --port 443 --certificates CertificateArn="$CERT_ARN" \
              --ssl-policy ELBSecurityPolicy-TLS13-1-2-2021-06 \
              --default-actions Type=forward,TargetGroupArn="$FRONTEND_TG_ARN" \
              --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" \
              --query "Listeners[0].ListenerArn" --output text --region "$AWS_REGION")
            echo "  âœ… Created HTTPS listener"

            # Add /api/* rule to HTTPS listener
            aws elbv2 create-rule --listener-arn "$HTTPS_LISTENER" --priority 10 \
              --conditions Field=path-pattern,Values='/api/*' \
              --actions Type=forward,TargetGroupArn="$BACKEND_TG_ARN" \
              --tags Key=Project,Value="$PROJECT_NAME" Key=Environment,Value="$ENVIRONMENT" \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "  âœ… Created HTTPS /api/* rule"
          else
            echo "  â­ï¸  HTTPS listener exists"
            # Ensure correct certificate
            aws elbv2 modify-listener --listener-arn "$HTTPS_LISTENER" \
              --certificates CertificateArn="$CERT_ARN" --region "$AWS_REGION" --no-cli-pager >/dev/null 2>&1 || true
          fi

          # HTTP â†’ HTTPS redirect
          CURRENT_ACTION=$(aws elbv2 describe-listeners --listener-arns "$LISTENER_ARN" \
            --region "$AWS_REGION" --query 'Listeners[0].DefaultActions[0].Type' --output text 2>/dev/null || echo "")

          if [[ "$CURRENT_ACTION" != "redirect" ]]; then
            aws elbv2 modify-listener --listener-arn "$LISTENER_ARN" \
              --default-actions 'Type=redirect,RedirectConfig={Protocol=HTTPS,Port=443,StatusCode=HTTP_301}' \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "  âœ… HTTP â†’ HTTPS redirect configured"
          else
            echo "  â­ï¸  HTTP redirect already configured"
          fi

          echo "backend_url=https://${FULL_DOMAIN}" >> "$GITHUB_OUTPUT"

      # â”€â”€ 9. DNS Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Setup DNS
        env:
          PROD_DOMAIN: ${{ secrets.PROD_DOMAIN || '' }}
        run: |
          echo "ğŸŒ Setting up DNS..."

          if [[ -z "$PROD_DOMAIN" ]]; then
            echo "  â­ï¸  PROD_DOMAIN not set â€” skipping DNS"
            exit 0
          fi

          # Prod uses apex domain
          FULL_DOMAIN="${PROD_DOMAIN}"
          ALB_DNS="${{ steps.alb.outputs.alb_dns }}"
          ALB_NAME="${PROJECT_NAME}-${ENVIRONMENT}-alb"

          # Find hosted zone
          ZONE_ID=$(aws route53 list-hosted-zones-by-name \
            --query "HostedZones[?Name=='${PROD_DOMAIN}.'].Id" \
            --output text 2>/dev/null | cut -d'/' -f3 || echo "")

          if [[ -z "$ZONE_ID" || "$ZONE_ID" == "None" ]]; then
            echo "  âš ï¸  No hosted zone found for ${PROD_DOMAIN} â€” skipping DNS"
            exit 0
          fi

          # Get ALB hosted zone ID
          ALB_ZONE_ID=$(aws elbv2 describe-load-balancers --names "$ALB_NAME" \
            --region "$AWS_REGION" --query "LoadBalancers[0].CanonicalHostedZoneId" --output text 2>/dev/null || echo "")

          if [[ -z "$ALB_ZONE_ID" || "$ALB_ZONE_ID" == "None" ]]; then
            echo "  âš ï¸  Could not get ALB zone ID â€” skipping DNS"
            exit 0
          fi

          # Upsert DNS record
          CHANGE_BATCH=$(cat <<EOF
          {
            "Changes": [{
              "Action": "UPSERT",
              "ResourceRecordSet": {
                "Name": "${FULL_DOMAIN}",
                "Type": "A",
                "AliasTarget": {
                  "HostedZoneId": "${ALB_ZONE_ID}",
                  "DNSName": "${ALB_DNS}",
                  "EvaluateTargetHealth": false
                }
              }
            }]
          }
          EOF
          )

          aws route53 change-resource-record-sets --hosted-zone-id "$ZONE_ID" \
            --change-batch "$CHANGE_BATCH" --no-cli-pager >/dev/null
          echo "  âœ… DNS record set: ${FULL_DOMAIN} â†’ ${ALB_DNS}"

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      #  BUILD & DEPLOY APPLICATION
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

      # â”€â”€ 10. Build & Push Images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Login to Amazon ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push backend image
        run: |
          ECR_BASE="${{ steps.vars.outputs.ecr_base }}"
          IMAGE_TAG="${{ steps.vars.outputs.image_tag }}"
          docker build \
            -t "${ECR_BASE}/${BACKEND_ECR_REPO}:${IMAGE_TAG}" \
            -t "${ECR_BASE}/${BACKEND_ECR_REPO}:${ENVIRONMENT}-latest" \
            -f backend/Dockerfile backend/
          docker push "${ECR_BASE}/${BACKEND_ECR_REPO}:${IMAGE_TAG}"
          docker push "${ECR_BASE}/${BACKEND_ECR_REPO}:${ENVIRONMENT}-latest"

      - name: Build and push frontend image
        run: |
          ECR_BASE="${{ steps.vars.outputs.ecr_base }}"
          IMAGE_TAG="${{ steps.vars.outputs.image_tag }}"
          BACKEND_URL="${{ steps.https.outputs.backend_url }}"

          # Fallback if HTTPS step didn't set backend_url
          if [[ -z "$BACKEND_URL" ]]; then
            BACKEND_URL="http://${{ steps.alb.outputs.alb_dns }}"
          fi

          docker build \
            --build-arg REACT_APP_BACKEND_URL="${BACKEND_URL}" \
            -t "${ECR_BASE}/${FRONTEND_ECR_REPO}:${IMAGE_TAG}" \
            -t "${ECR_BASE}/${FRONTEND_ECR_REPO}:${ENVIRONMENT}-latest" \
            -f frontend/Dockerfile frontend/
          docker push "${ECR_BASE}/${FRONTEND_ECR_REPO}:${IMAGE_TAG}"
          docker push "${ECR_BASE}/${FRONTEND_ECR_REPO}:${ENVIRONMENT}-latest"

      # â”€â”€ 11. Register Task Definitions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Register backend task definition
        run: |
          ACCOUNT_ID="${{ steps.vars.outputs.account_id }}"
          ECR_BASE="${{ steps.vars.outputs.ecr_base }}"
          SECRET_ARN="${{ steps.secrets.outputs.secret_arn }}"
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"

          EXEC_ROLE_ARN=$(aws iam get-role --role-name "${APP_NAME}-ecs-task-execution-role" \
            --query 'Role.Arn' --output text)
          TASK_ROLE_ARN=$(aws iam get-role --role-name "${APP_NAME}-ecs-task-role" \
            --query 'Role.Arn' --output text)

          aws ecs register-task-definition \
            --family "${APP_NAME}-backend-task" \
            --network-mode awsvpc --requires-compatibilities FARGATE \
            --cpu 512 --memory 1024 \
            --execution-role-arn "$EXEC_ROLE_ARN" --task-role-arn "$TASK_ROLE_ARN" \
            --container-definitions "[{
              \"name\": \"backend\",
              \"image\": \"${ECR_BASE}/${BACKEND_ECR_REPO}:${ENVIRONMENT}-latest\",
              \"essential\": true,
              \"portMappings\": [{\"containerPort\": 8001, \"protocol\": \"tcp\"}],
              \"secrets\": [{\"name\": \"GROQ_API_KEY\", \"valueFrom\": \"${SECRET_ARN}:GROQ_API_KEY::\"}],
              \"environment\": [
                {\"name\": \"DYNAMODB_TABLE_NAME\", \"value\": \"${PROJECT_NAME}-${ENVIRONMENT}\"},
                {\"name\": \"AWS_DEFAULT_REGION\", \"value\": \"${AWS_REGION}\"},
                {\"name\": \"ENVIRONMENT\", \"value\": \"${ENVIRONMENT}\"},
                {\"name\": \"ENABLE_CLOUDWATCH_METRICS\", \"value\": \"true\"}
              ],
              \"logConfiguration\": {
                \"logDriver\": \"awslogs\",
                \"options\": {
                  \"awslogs-group\": \"/ecs/${APP_NAME}-backend\",
                  \"awslogs-region\": \"${AWS_REGION}\",
                  \"awslogs-stream-prefix\": \"ecs\"
                }
              },
              \"healthCheck\": {
                \"command\": [\"CMD-SHELL\", \"python -c \\\"import urllib.request; exit(0 if urllib.request.urlopen('http://localhost:8001/api/health').status == 200 else 1)\\\"\"],
                \"interval\": 30, \"timeout\": 10, \"retries\": 3, \"startPeriod\": 60
              }
            }]" \
            --tags "key=Project,value=$PROJECT_NAME" "key=Environment,value=$ENVIRONMENT" \
            --region "$AWS_REGION" --no-cli-pager >/dev/null
          echo "âœ… Registered backend task definition"

      - name: Register frontend task definition
        run: |
          ACCOUNT_ID="${{ steps.vars.outputs.account_id }}"
          ECR_BASE="${{ steps.vars.outputs.ecr_base }}"
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"

          EXEC_ROLE_ARN=$(aws iam get-role --role-name "${APP_NAME}-ecs-task-execution-role" \
            --query 'Role.Arn' --output text)
          TASK_ROLE_ARN=$(aws iam get-role --role-name "${APP_NAME}-ecs-task-role" \
            --query 'Role.Arn' --output text)

          aws ecs register-task-definition \
            --family "${APP_NAME}-frontend-task" \
            --network-mode awsvpc --requires-compatibilities FARGATE \
            --cpu 256 --memory 512 \
            --execution-role-arn "$EXEC_ROLE_ARN" --task-role-arn "$TASK_ROLE_ARN" \
            --container-definitions "[{
              \"name\": \"frontend\",
              \"image\": \"${ECR_BASE}/${FRONTEND_ECR_REPO}:${ENVIRONMENT}-latest\",
              \"essential\": true,
              \"portMappings\": [{\"containerPort\": 8080, \"protocol\": \"tcp\"}],
              \"logConfiguration\": {
                \"logDriver\": \"awslogs\",
                \"options\": {
                  \"awslogs-group\": \"/ecs/${APP_NAME}-frontend\",
                  \"awslogs-region\": \"${AWS_REGION}\",
                  \"awslogs-stream-prefix\": \"ecs\"
                }
              },
              \"healthCheck\": {
                \"command\": [\"CMD-SHELL\", \"wget --quiet --tries=1 --spider http://localhost:8080/ || exit 1\"],
                \"interval\": 30, \"timeout\": 5, \"retries\": 3, \"startPeriod\": 30
              }
            }]" \
            --tags "key=Project,value=$PROJECT_NAME" "key=Environment,value=$ENVIRONMENT" \
            --region "$AWS_REGION" --no-cli-pager >/dev/null
          echo "âœ… Registered frontend task definition"

      # â”€â”€ 12. Create/Update ECS Services â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Create or update ECS services
        run: |
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"
          CLUSTER="${APP_NAME}-cluster"
          ECS_SG="${{ steps.networking.outputs.ecs_sg_id }}"
          PUB1="${{ steps.networking.outputs.pub_subnet_1 }}"
          PUB2="${{ steps.networking.outputs.pub_subnet_2 }}"
          BACKEND_TG="${{ steps.alb.outputs.backend_tg_arn }}"
          FRONTEND_TG="${{ steps.alb.outputs.frontend_tg_arn }}"

          # Prod uses 2 tasks for high availability
          DESIRED_COUNT=2

          # Backend service
          EXISTING=$(aws ecs describe-services --cluster "$CLUSTER" --services "${APP_NAME}-backend" \
            --query "services[?status=='ACTIVE'].serviceName" --output text \
            --region "$AWS_REGION" 2>/dev/null || echo "")

          if [[ -n "$EXISTING" && "$EXISTING" != "None" ]]; then
            aws ecs update-service --cluster "$CLUSTER" --service "${APP_NAME}-backend" \
              --task-definition "${APP_NAME}-backend-task" --desired-count "$DESIRED_COUNT" --force-new-deployment \
              --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100" \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "âœ… Updated backend service"
          else
            aws ecs create-service --cluster "$CLUSTER" --service-name "${APP_NAME}-backend" \
              --task-definition "${APP_NAME}-backend-task" --desired-count "$DESIRED_COUNT" --launch-type FARGATE \
              --network-configuration "awsvpcConfiguration={subnets=[$PUB1,$PUB2],securityGroups=[$ECS_SG],assignPublicIp=ENABLED}" \
              --load-balancers "targetGroupArn=$BACKEND_TG,containerName=backend,containerPort=8001" \
              --health-check-grace-period-seconds 120 \
              --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100" \
              --tags key=Project,value="$PROJECT_NAME" key=Environment,value="$ENVIRONMENT" \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "âœ… Created backend service"
          fi

          # Frontend service
          EXISTING=$(aws ecs describe-services --cluster "$CLUSTER" --services "${APP_NAME}-frontend" \
            --query "services[?status=='ACTIVE'].serviceName" --output text \
            --region "$AWS_REGION" 2>/dev/null || echo "")

          if [[ -n "$EXISTING" && "$EXISTING" != "None" ]]; then
            aws ecs update-service --cluster "$CLUSTER" --service "${APP_NAME}-frontend" \
              --task-definition "${APP_NAME}-frontend-task" --desired-count "$DESIRED_COUNT" --force-new-deployment \
              --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100" \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "âœ… Updated frontend service"
          else
            aws ecs create-service --cluster "$CLUSTER" --service-name "${APP_NAME}-frontend" \
              --task-definition "${APP_NAME}-frontend-task" --desired-count "$DESIRED_COUNT" --launch-type FARGATE \
              --network-configuration "awsvpcConfiguration={subnets=[$PUB1,$PUB2],securityGroups=[$ECS_SG],assignPublicIp=ENABLED}" \
              --load-balancers "targetGroupArn=$FRONTEND_TG,containerName=frontend,containerPort=8080" \
              --health-check-grace-period-seconds 60 \
              --deployment-configuration "maximumPercent=200,minimumHealthyPercent=100" \
              --tags key=Project,value="$PROJECT_NAME" key=Environment,value="$ENVIRONMENT" \
              --region "$AWS_REGION" --no-cli-pager >/dev/null
            echo "âœ… Created frontend service"
          fi

      # â”€â”€ 13. Async Deployment Callback (LEAN WAIT) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Provision async deployment callback infrastructure
        env:
          GH_TOKEN: ${{ secrets.GH_CALLBACK_TOKEN }}
        run: |
          # Only provision if callback token is available
          if [[ -z "$GH_TOKEN" ]]; then
            echo "â­ï¸  GH_CALLBACK_TOKEN is not set. Skipping callback infrastructure setup."
            echo "Deployment will still occur, but without the automated async callback job."
            exit 0
          fi

          echo "ğŸš€ Provisioning EventBridge-to-GitHub callback infrastructure..."
          APP_NAME="${PROJECT_NAME}-${ENVIRONMENT}"
          
          # best-effort self-grant of EventBridge/IAM permissions (setup runs with
          # the same credentials that will be used for deployments)
          if aws iam get-user &>/dev/null 2>&1; then
            USERNAME=$(aws iam get-user --query 'User.UserName' --output text)
            # Full permission block required for EventBridge to manage its own connections (incl. Secrets Manager)
            CALLBACK_POLICY='{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Action":["events:CreateConnection","events:DescribeConnection","events:UpdateConnection","events:CreateApiDestination","events:DescribeApiDestination","events:UpdateApiDestination","events:PutRule","events:PutTargets","iam:GetRole","iam:CreateRole","iam:PutRolePolicy","iam:PutUserPolicy","iam:PassRole","iam:CreateServiceLinkedRole","secretsmanager:CreateSecret","secretsmanager:TagResource","secretsmanager:GetSecretValue","secretsmanager:PutSecretValue","secretsmanager:DescribeSecret","secretsmanager:UpdateSecret","secretsmanager:DeleteSecret"],"Resource":"*"}]}'
            aws iam put-user-policy --user-name "$USERNAME" --policy-name "ShoreExplorerCallbackHelper" --policy-document "$CALLBACK_POLICY" >/dev/null 2>&1 || true
            # Wait for IAM inline-policy propagation before proceeding
            sleep 15
          fi

          # helper for permission errors
          fail() {
            echo "âŒ $1"
            echo "   Error Details: ${2:-No additional info}"
            echo "   Deployment will continue, but the async callback won't be configured."
            # We exit 0 because if callback infrastructure fails, we don't want to break the whole deployment.
            exit 0
          }

          # 1. Connection (Auth for GitHub)
          CONN_ARN=$(aws events describe-connection --name "${APP_NAME}-gh-conn" \
            --query 'ConnectionArn' --output text --region "$AWS_REGION" 2>/dev/null || echo "")
          
          if [[ -z "$CONN_ARN" || "$CONN_ARN" == "None" ]]; then
            if ! CONN_OUTPUT=$(aws events create-connection --name "${APP_NAME}-gh-conn" \
              --authorization-type API_KEY \
              --auth-parameters "{\"ApiKeyAuthParameters\": {\"ApiKeyName\": \"Authorization\", \"ApiKeyValue\": \"Bearer ${GH_TOKEN}\"}}" \
              --region "$AWS_REGION" 2>&1); then
              fail "Unable to create EventBridge Connection. Ensure deployment role has events:CreateConnection and related Secrets Manager actions." "$CONN_OUTPUT"
            fi
            CONN_ARN=$(echo "$CONN_OUTPUT" | grep -o 'arn:aws:events:[^"]*')
            echo "  âœ… Created EventBridge Connection"
          else
            echo "  â­ï¸  Connection exists"
          fi

          # 2. API Destination (GitHub Repos Dispatch)
          DEST_ARN=$(aws events describe-api-destination --name "${APP_NAME}-gh-dispatch" \
            --query 'ApiDestinationArn' --output text --region "$AWS_REGION" 2>/dev/null || echo "")
          
          if [[ -z "$DEST_ARN" || "$DEST_ARN" == "None" ]]; then
            DEST_URL="https://api.github.com/repos/${{ github.repository }}/dispatches"
            if ! DEST_OUTPUT=$(aws events create-api-destination --name "${APP_NAME}-gh-dispatch" \
              --connection-arn "$CONN_ARN" \
              --invocation-endpoint "$DEST_URL" \
              --http-method POST \
              --region "$AWS_REGION" 2>&1); then
              fail "Unable to create API Destination. Ensure deployment role has events:CreateApiDestination." "$DEST_OUTPUT"
            fi
            DEST_ARN=$(echo "$DEST_OUTPUT" | grep -o 'arn:aws:events:[^"]*')
            echo "  âœ… Created API Destination"
          else
            echo "  â­ï¸  API Destination exists"
          fi

          # 3. IAM Role for EventBridge to call API Destination
          ROLE_NAME="${APP_NAME}-eventbridge-callback-role"
          if ! aws iam get-role --role-name "$ROLE_NAME" &>/dev/null; then
            TRUST_POLICY='{"Version":"2012-10-17","Statement":[{"Effect":"Allow","Principal":{"Service":"events.amazonaws.com"},"Action":"sts:AssumeRole"}]}'
            if ! aws iam create-role --role-name "$ROLE_NAME" --assume-role-policy-document "$TRUST_POLICY" >/dev/null 2>&1; then
              fail "Unable to create IAM role. Ensure deployment role has iam:CreateRole."
            fi
            
            POLICY_DOC="{\"Version\":\"2012-10-17\",\"Statement\":[{\"Effect\":\"Allow\",\"Action\":\"events:InvokeApiDestination\",\"Resource\":\"$DEST_ARN\"}]}"
            if ! aws iam put-role-policy --role-name "$ROLE_NAME" --policy-name "InvokeAPIDestination" --policy-document "$POLICY_DOC" >/dev/null 2>&1; then
              fail "Unable to attach policy to callback IAM role. Ensure deployment role has iam:PutRolePolicy."
            fi
            echo "  âœ… Created IAM Role for EventBridge callback"
          fi
          ROLE_ARN=$(aws iam get-role --role-name "$ROLE_NAME" --query 'Role.Arn' --output text)

          # 4. Rule (Listen for SUCCESS)
          aws events put-rule --name "${APP_NAME}-ecs-success-rule" \
            --event-pattern "{\"source\":[\"aws.ecs\"],\"detail-type\":[\"ECS Service Action\"],\"detail\":{\"clusterArn\":[\"arn:aws:ecs:${AWS_REGION}:${{ steps.vars.outputs.account_id }}:cluster/${APP_NAME}-cluster\"],\"eventName\":[\"SERVICE_STEADY_STATE\"]}}" \
            --state ENABLED --region "$AWS_REGION" --no-cli-pager >/dev/null
          
          aws events put-targets --rule "${APP_NAME}-ecs-success-rule" --region "$AWS_REGION" \
            --targets "[{\"Id\":\"GitHubSuccessDispatch\",\"Arn\":\"$DEST_ARN\",\"RoleArn\":\"$ROLE_ARN\",\"HttpParameters\":{\"QueryStringParameters\":{},\"HeaderParameters\":{\"User-Agent\":\"AWS-EventBridge\"}},\"InputTransformer\":{\"InputPathsMap\":{\"environment\":\"$.detail.clusterArn\"},\"InputTemplate\":\"{\\\"event_type\\\": \\\"ecs_deploy_success\\\", \\\"client_payload\\\": {\\\"environment\\\": \\\"${ENVIRONMENT}\\\", \\\"status\\\": \\\"stable\\\"}}\"}}]" \
            --no-cli-pager >/dev/null
          echo "  âœ… Configured Success Callback Route"

          # 5. Rule (Listen for FAILURE)
          aws events put-rule --name "${APP_NAME}-ecs-failure-rule" \
            --event-pattern "{\"source\":[\"aws.ecs\"],\"detail-type\":[\"ECS Service Action\"],\"detail\":{\"clusterArn\":[\"arn:aws:ecs:${AWS_REGION}:${{ steps.vars.outputs.account_id }}:cluster/${APP_NAME}-cluster\"],\"eventName\":[\"SERVICE_DEPLOYMENT_FAILED\"]}}" \
            --state ENABLED --region "$AWS_REGION" --no-cli-pager >/dev/null
          
          aws events put-targets --rule "${APP_NAME}-ecs-failure-rule" --region "$AWS_REGION" \
            --targets "[{\"Id\":\"GitHubFailureDispatch\",\"Arn\":\"$DEST_ARN\",\"RoleArn\":\"$ROLE_ARN\",\"HttpParameters\":{\"QueryStringParameters\":{},\"HeaderParameters\":{\"User-Agent\":\"AWS-EventBridge\"}},\"InputTransformer\":{\"InputPathsMap\":{\"environment\":\"$.detail.clusterArn\"},\"InputTemplate\":\"{\\\"event_type\\\": \\\"ecs_deploy_failed\\\", \\\"client_payload\\\": {\\\"environment\\\": \\\"${ENVIRONMENT}\\\", \\\"status\\\": \\\"failed\\\"}}\"}}]" \
            --no-cli-pager >/dev/null
          echo "  âœ… Configured Failure Callback Route"

      # â”€â”€ 14. Monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Setup monitoring
        env:
          ALERT_EMAIL: ${{ secrets.ALERT_EMAIL || '' }}
        run: |
          chmod +x infra/aws/scripts/10-setup-monitoring.sh
          bash infra/aws/scripts/10-setup-monitoring.sh "${ENVIRONMENT}"
        continue-on-error: true

      # â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Deployment summary
        if: always()
        run: |
          BACKEND_URL="${{ steps.https.outputs.backend_url }}"
          if [[ -z "$BACKEND_URL" ]]; then
            BACKEND_URL="http://${{ steps.alb.outputs.alb_dns }}"
          fi

          echo "## Production Environment Setup & Deploy Summary" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "| Item | Value |" >> "$GITHUB_STEP_SUMMARY"
          echo "|------|-------|" >> "$GITHUB_STEP_SUMMARY"
          echo "| Environment | \`production\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Image Tag | \`${{ steps.vars.outputs.image_tag }}\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Commit | \`${{ github.sha }}\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| App URL | ${BACKEND_URL} |" >> "$GITHUB_STEP_SUMMARY"
          echo "| API Health | ${BACKEND_URL}/api/health |" >> "$GITHUB_STEP_SUMMARY"
          DASHBOARD_URL="https://${AWS_REGION}.console.aws.amazon.com/cloudwatch/home?region=${AWS_REGION}#dashboards:name=${PROJECT_NAME}-${ENVIRONMENT}-dashboard"
          echo "| Dashboard | [CloudWatch](${DASHBOARD_URL}) |" >> "$GITHUB_STEP_SUMMARY"

  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  # Post-deployment cleanup and smoke tests
  # Runs ONCE after both services have deployed
  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  cleanup-and-smoke:
    name: Deployment Cleanup & Smoke Test
    runs-on: ubuntu-latest
    needs: setup-and-deploy-prod
    if: always()
    environment: production-post-deploy

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get deployment URL
        id: alb
        env:
          CUSTOM_DOMAIN: ${{ secrets.PROD_DOMAIN || '' }}
        run: |
          ALB_DNS=$(aws elbv2 describe-load-balancers \
            --names "${PROJECT_NAME}-${ENVIRONMENT}-alb" \
            --query "LoadBalancers[0].DNSName" \
            --output text --region "$AWS_REGION" 2>/dev/null || echo "")

          if [[ -n "$CUSTOM_DOMAIN" ]]; then
            URL="https://${CUSTOM_DOMAIN}"
          elif [[ -n "$ALB_DNS" && "$ALB_DNS" != "None" ]]; then
            URL="https://${ALB_DNS}"
          else
            URL="http://localhost:8001"
          fi
          echo "url=${URL}" >> "$GITHUB_OUTPUT"

      - name: Wait for ECS services to stabilize
        if: needs.setup-and-deploy-prod.result == 'success'
        run: |
          echo "Waiting for ECS services to stabilize (up to 10 minutes)..."
          CLUSTER="${PROJECT_NAME}-${ENVIRONMENT}-cluster"
          SERVICES=(
            "${PROJECT_NAME}-${ENVIRONMENT}-backend"
            "${PROJECT_NAME}-${ENVIRONMENT}-frontend"
          )
          MAX_WAIT=600
          POLL_INTERVAL=15
          elapsed=0

          while [[ $elapsed -lt $MAX_WAIT ]]; do
            all_stable=true
            for svc in "${SERVICES[@]}"; do
              svc_info=$(aws ecs describe-services \
                --cluster "$CLUSTER" --services "$svc" --region "$AWS_REGION" \
                --query "services[0].{active: length(deployments[?status!='INACTIVE']), running: runningCount, desired: desiredCount}" \
                --output json)
              active=$(echo "$svc_info" | jq -r '.active')
              running=$(echo "$svc_info" | jq -r '.running')
              desired=$(echo "$svc_info" | jq -r '.desired')
              echo "${svc}: active_deployments=${active}, running=${running}, desired=${desired}"
              if [[ "$active" != "1" || "$running" != "$desired" ]]; then
                all_stable=false
              fi
            done

            if $all_stable; then
              echo "âœ… All production services stable!"
              break
            fi

            sleep $POLL_INTERVAL
            elapsed=$((elapsed + POLL_INTERVAL))
          done

          if [[ $elapsed -ge $MAX_WAIT ]]; then
            echo "âŒ Timed out after ${MAX_WAIT}s waiting for production services to stabilize"
            exit 1
          fi

      - name: Run smoke tests
        if: needs.setup-and-deploy-prod.result == 'success' && steps.alb.outputs.url != 'http://localhost:8001'
        run: |
          URL="${{ steps.alb.outputs.url }}"
          echo "Running smoke tests against ${URL}"
          sleep 15

          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            "${URL}/api/health" --max-time 30 --retry 3 --retry-delay 10 || echo "000")
          if [[ "$HTTP_CODE" == "200" ]]; then
            echo "âœ… Health check passed (HTTP $HTTP_CODE)"
          else
            echo "âŒ Health check failed (HTTP $HTTP_CODE)"
            exit 1
          fi

          HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
            "${URL}/" --max-time 30 --retry 3 --retry-delay 10 || echo "000")
          if [[ "$HTTP_CODE" == "200" ]]; then
            echo "âœ… Frontend accessible (HTTP $HTTP_CODE)"
          else
            echo "âŒ Frontend check failed (HTTP $HTTP_CODE)"
            exit 1
          fi

      - name: Smoke test & cleanup summary
        if: always()
        run: |
          DEPLOY_STATUS="${{ needs.setup-and-deploy-prod.result }}"
          echo "## Deployment Cleanup & Smoke Test" >> "$GITHUB_STEP_SUMMARY"
          echo "" >> "$GITHUB_STEP_SUMMARY"
          echo "| Item | Value |" >> "$GITHUB_STEP_SUMMARY"
          echo "|------|-------|" >> "$GITHUB_STEP_SUMMARY"
          echo "| Environment | \`${ENVIRONMENT}\` |" >> "$GITHUB_STEP_SUMMARY"
          echo "| Setup & Deploy Result | \`${DEPLOY_STATUS}\` |" >> "$GITHUB_STEP_SUMMARY"
          if [[ "${{ steps.alb.outputs.url }}" != "" && "${{ steps.alb.outputs.url }}" != "http://localhost:8001" ]]; then
            echo "| App URL | ${{ steps.alb.outputs.url }} |" >> "$GITHUB_STEP_SUMMARY"
            echo "| API Health | ${{ steps.alb.outputs.url }}/api/health |" >> "$GITHUB_STEP_SUMMARY"
          fi
